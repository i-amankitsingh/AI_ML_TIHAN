# -*- coding: utf-8 -*-
"""4th Sep .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l8mdEl3sLPSvFr9_lACkjWxYTMrWSBXQ
"""

"""In-Class Notes

Topic: Introduction to Autonomous Drones

1. History & Evolution of Aerial Vehicles

*   Ancient roots: Icarus (Greek myth), Weimanika Shastra (India).
*   Early milestones:
    *   1783: Hot air balloons – Montgolfier brothers.
    *   1852: First powered airship – Henri Giffard.
    *   Otto Lilienthal – gliders, bird-like designs.
    *   Wright brothers – first controlled powered flight (1906).
*   Modern drones = Mechanics + Electronics + Computer science.

2. Physics of Flight

*   4 Key Forces:
    *   Lift → wings/propellers, Bernoulli’s principle.
    *   Thrust → engines/propellers.
    *   Weight → gravity, acts downward.
    *   Drag → resistance, opposes motion.
*   Axes of motion:
    *   Roll (longitudinal), Pitch (lateral), Yaw (normal).
*   Quadcopter control = adjusting propeller speeds.
*   Stability concepts: Center of Gravity (CG), Center of Pressure (CP).

3. Drone Configurations

*   Monocopter → simple, limited control.
*   Bicopter → two props, yaw control.
*   Tricopter → 3 props, better load capacity.
*   Quadcopter → most common, stable.
*   Hexacopter → 6 props, heavier loads, motor-failure tolerance.
*   Octocopter → 8 props, high power + payload.
*   EVTOL fixed-wing → hybrid endurance drones.
*   Ornithopters → biomimicry, flapping wings.

4. Basic Components

*   Propellers → thrust/lift via airflow.
*   BLDC motors → permanent magnet motors, KV rating = RPM/Volt.
*   ESC (Electronic Speed Controller) → adjusts motor speed (PWM signals).
*   Flight Controller → "brain" (Pixhawk, Orange Cube).
*   Frame → carbon fiber/plastic/aluminum.
*   Battery → LiPo/Li-ion; mAh (capacity), Voltage (cells), C-rating (discharge).
*   Power Distribution Board → connects battery to motors/electronics.
*   GPS Module → positioning/navigation (limitations: jamming).
*   Transmitter & Receiver → remote communication, 2.4 GHz band.

5. Drone Design Process

*   Define requirements (payload, endurance, range).
*   Estimate total weight.
*   Select thrust-to-weight ratio (e.g., 2:1 for surveillance drones).
*   Choose frame type (quad/hexa/octa).
*   Calculate thrust per motor.
*   Select motors & propellers via market survey.
*   Choose ESC based on current & voltage.
*   Select battery → matches current & flight time.
*   Pick flight controller & transmitter.
*   Balance weight, stability, endurance, cost.
*   Case Study → Agricultural drone carrying 15 kg, 15 min endurance, 5 km range.

6. Key Takeaways

*   Drones = blend of aerospace + AI + electronics.
*   Design = iterative & practical (balance performance vs. cost).
*   Stability & power management are crucial.
*   Indigenous subsystems = sustainability & competitiveness.
*   Strong link between physics (forces, aerodynamics) and practical design choices.

"""# Machine Learning and Deep Learning

Here are examples demonstrating the basic usage of Scikit-learn, TensorFlow, and PyTorch for a simple task like linear regression.

### Scikit-learn Example: Linear Regression
"""

from sklearn.linear_model import LinearRegression
import numpy as np

# Sample data
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 5, 4])

# Create and train the model
model = LinearRegression()
model.fit(X, y)

# Make a prediction
prediction = model.predict([[5]])
print(f"Scikit-learn Prediction: {prediction[0]:.2f}")

"""**Explanation:**
This code performs a simple linear regression using Scikit-learn. It defines input data `X` and output data `y`, creates a `LinearRegression` model, trains it using the `fit` method, and then makes a prediction for a new input using the `predict` method.

**What differentiates Scikit-learn:**
*   **Ease of Use:** Simple and consistent API for various algorithms.
*   **Classical ML Focus:** Primarily for traditional machine learning tasks.
*   **Great for Beginners and Prototyping:** Excellent for getting started and quick experiments.

### TensorFlow Example: Linear Regression
"""

#deep learing
import tensorflow as tf
import numpy as np

# Sample data
X = np.array([[1.], [2.], [3.], [4.]], dtype=np.float32)
y = np.array([2., 4., 5., 4.], dtype=np.float32)

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1, input_shape=[1])
])

# Compile and train the model
model.compile(loss='mean_squared_error', optimizer=tf.optimizers.Adam(0.1))
model.fit(X, y, epochs=100, verbose=0)

# Make a prediction
prediction = model.predict(tf.constant([[5.]], dtype=tf.float32))
print(f"TensorFlow Prediction: {prediction[0][0]:.2f}")

"""**Explanation:**
This example uses TensorFlow with the Keras API to build and train a simple linear regression model. It defines a sequential model with a dense layer, compiles it with a loss function and optimizer, trains it using the `fit` method, and makes a prediction with `predict`.

**What differentiates TensorFlow:**
*   **Production Readiness:** Strong support for deployment and scaling.
*   **Static and Dynamic Graphs:** Offers flexibility in model building.
*   **Comprehensive Ecosystem:** Tools for data processing, deployment, and more.

### PyTorch Example: Linear Regression
"""

#deep learing

import torch
import torch.nn as nn
import numpy as np

# Sample data
X = torch.tensor([[1.], [2.], [3.], [4.]], dtype=torch.float32)
y = torch.tensor([[2.], [4.], [5.], [4.]], dtype=torch.float32)

# Define the model
model = nn.Linear(1, 1)

# Define loss and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Train the model
epochs = 100
for epoch in range(epochs):
    outputs = model(X)
    loss = criterion(outputs, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Make a prediction
with torch.no_grad():
    prediction = model(torch.tensor([[5.]], dtype=torch.float32))
    print(f"PyTorch Prediction: {prediction.item():.2f}")

"""**Explanation:**
This PyTorch example demonstrates building and training a linear regression model. It defines the model as an `nn.Linear` layer, sets up a loss function (`MSELoss`) and optimizer (`SGD`), and trains the model in a loop by performing forward passes, calculating the loss, backpropagating gradients, and updating weights.

**What differentiates PyTorch:**
*   **Pythonic and Flexible:** More intuitive and easier for dynamic model building.
*   **Research-Oriented:** Popular in the research community due to its flexibility.
*   **Dynamic Computation Graph:** Allows for more dynamic and interactive development.
"""

import tensorflow as tf
import numpy as np

# Sample data
X = np.array([[1.], [2.], [3.], [4.]], dtype=np.float32)
y = np.array([2., 4., 5., 4.], dtype=np.float32)

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1, input_shape=[1])
])

# Compile and train the model
model.compile(loss='mean_squared_error', optimizer=tf.optimizers.Adam(0.1))
model.fit(X, y, epochs=100, verbose=0)

# Make a prediction
prediction = model.predict(tf.constant([[5.]], dtype=tf.float32))
print(f"TensorFlow Prediction: {prediction[0][0]:.2f}")

"""## Scaling Techniques in Machine Learning

Scaling is an important preprocessing step to ensure that features are on a similar scale, which can improve the performance of many machine learning algorithms.

### 1. Normalization (Min-Max Scaling)
"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Sample data (features)
X = np.array([[0], [2], [300], [5], [50]])

# Apply Min-Max Scaling
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

print("Original Data:\n", X)
print("Normalized Data (Min-Max Scaling):\n", X_scaled)

"""This scales features to a fixed range, usually 0 to 1.
It is sensitive to outliers, which can be compressed into a small range.

### 2. Standardization (Z-score Scaling)
"""

from sklearn.preprocessing import StandardScaler
import numpy as np

# Sample data (features)
X = np.array([[0], [2], [3], [4], [10]])

# Apply Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Original Data:\n", X)
print("Standardized Data (Z-score Scaling):\n", X_scaled)

"""This scales features to have a mean of 0 and a standard deviation of 1.
It is less affected by outliers compared to Min-Max Scaling.

If an original data point is exactly the average, its standardized value will be 0.
If an original data point is above the average, its standardized value will be positive. The larger the positive number, the further above the average it is.

If an original data point is below the average, its standardized value will be negative. The larger the negative number (in absolute value), the further below the average it is.

So, the standardized data is just a transformed version of the original data, where the values now represent their position relative to the mean of the original data, measured in units of the original data's standard deviation. The shape of the data's distribution stays the same, but it's shifted and scaled so the new average is 0 and the new spread (standard deviation) is 1.

### 3. Quantile Transformer
"""

from sklearn.preprocessing import QuantileTransformer
import numpy as np

# Sample data (features)
X = np.array([[1], [2], [3], [4], [10], [100], [200]]) # More varied data

# Apply Quantile Transformation (to normal distribution)
scaler = QuantileTransformer(output_distribution='normal', n_quantiles=5)
X_trans = scaler.fit_transform(X)

print("Original Data:\n", X)
print("Quantile Transformed Data (Normal Distribution):\n", X_trans)

"""This transforms features to follow a uniform or normal distribution.
It is useful for handling skewed data and is robust to outliers.

### When not to scale

Scaling is generally not required for tree-based algorithms like Decision Trees, Random Forests, and gradient boosting models (e.g., XGBoost, CatBoost). These algorithms are not sensitive to the scale of features.

## Encoding Categorical Features

Many machine learning algorithms require numerical input. Categorical features (like "color" or "city") need to be converted into numerical representations through a process called encoding.

### 1. Label Encoding
"""

from sklearn.preprocessing import LabelEncoder
import numpy as np

# Sample categorical data
y_cat = np.array(['red', 'blue', 'green', 'red', 'blue'])

# Apply Label Encoding
le = LabelEncoder()
y_encoded = le.fit_transform(y_cat)

print("Original Categorical Data:\n", y_cat)
print("Label Encoded Data:\n", y_encoded)

"""Assigns a unique integer to each category. Suitable for ordinal data or as a first step for tree-based models.
Can impose artificial order on non-ordinal data, which can negatively impact some models.

### 2. One-Hot Encoding
"""

from sklearn.preprocessing import OneHotEncoder
import numpy as np

# Sample categorical data
X_cat = np.array([['red'], ['blue'], ['green'], ['red'], ['blue']])

# Apply One-Hot Encoding
# sparse=False returns a dense NumPy array
ohe = OneHotEncoder(sparse_output=False)
X_ohe = ohe.fit_transform(X_cat)

print("Original Categorical Data:\n", X_cat)
print("One-Hot Encoded Data:\n", X_ohe)

"""Creates a binary column for each category. Ideal for nominal data to avoid imposing order.
Can lead to a high-dimensional feature space if there are many unique categories.

### 3. Ordinal Encoding
"""

from sklearn.preprocessing import OrdinalEncoder
import numpy as np

# Sample ordinal categorical data (e.g., size)
X_ord_cat = np.array([['small'], ['medium'], ['large'], ['medium'], ['small']])

# Define the order of categories
categories_order = [['medium', 'small', 'large']]

# Apply Ordinal Encoding
ord_enc = OrdinalEncoder(categories=categories_order)
X_ord_encoded = ord_enc.fit_transform(X_ord_cat)

print("Original Ordinal Data:\n", X_ord_cat)
print("Ordinal Encoded Data:\n", X_ord_encoded)

"""Maps categories to integers while preserving their inherent order. Requires specifying the order of categories.
Only suitable when a meaningful order exists between categories.

### 4. Target Encoding

Target encoding replaces each category with the mean of the target variable for that category. This often requires the `category_encoders` library.
"""

!pip install category_encoders

import category_encoders as ce
import numpy as np
import pandas as pd

# Sample categorical feature and a numerical target variable
X_cat = pd.DataFrame({'Color': ['red', 'blue', 'green', 'red', 'blue', 'green', 'red']})
y_target = pd.Series([10, 15, 12, 11, 16, 14, 9]) # Example numerical target

# Apply Target Encoding
encoder = ce.TargetEncoder(cols=['Color'])
X_target_encoded = encoder.fit_transform(X_cat, y_target)

print("Original Data:")
print(X_cat)
print("\nTarget Data:")
print(y_target)
print("\nTarget Encoded Data:")
print(X_target_encoded)

"""Replaces categories with a statistic based on the target variable. Can reduce dimensionality and capture information about the target.
Requires careful handling to avoid target leakage (using validation sets for encoding).

### 5. Binary Encoding

Binary encoding converts categories to binary digits. It's a compromise between Label Encoding and One-Hot Encoding, reducing dimensionality compared to One-Hot Encoding while avoiding the artificial order of Label Encoding. This can also be done with the `category_encoders` library.

### 5. Binary Encoding

Binary encoding converts categories to binary digits. It's a compromise between Label Encoding and One-Hot Encoding, reducing dimensionality compared to One-Hot Encoding while avoiding the artificial order of Label Encoding. This can also be done with the `category_encoders` library.
"""

import category_encoders as ce
import numpy as np
import pandas as pd

# Sample categorical data
X_cat = pd.DataFrame({'Color': ['red', 'blue', 'green', 'red', 'blue', 'green', 'red', 'yellow']})

print("Original Data:")
print(X_cat)
print("-" * 30) # Separator for clarity

# Apply Binary Encoding
# This will create new columns representing the binary code for each color.
# The number of new columns will be log2(number of unique colors).
encoder = ce.BinaryEncoder(cols=['Color'])
X_binary_encoded = encoder.fit_transform(X_cat)

print("Binary Encoded Data:")
print(X_binary_encoded)

print("-" * 30) # Separator for clarity
print("Explanation of Binary Encoding:")
print("Each unique color is assigned a unique binary code.")
print("The columns Color_0, Color_1, etc. represent the binary digits.")
print("For example, if 'red' is encoded as 001:")
print("- Rows with 'red' in 'Original Data' will have 0 in 'Color_0', 0 in 'Color_1', and 1 in 'Color_2' in 'Binary Encoded Data'.")
print("Observe the patterns in the 'Binary Encoded Data' to see the binary codes for each color.")

# 1. Linear Regression (Scikit-learn)
from sklearn.linear_model import LinearRegression
import numpy as np

X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 5, 4])

model = LinearRegression()
model.fit(X, y)
prediction = model.predict([[5]])
print(f"Prediction: {prediction[0]:.2f}")

"""*   Uses Scikit-learn for a simple linear regression model.
*   Trains the model on sample data and makes a prediction.
*   Demonstrates the ease of use of Scikit-learn for classical ML.
"""

# 2. Standardization (Z-score Scaling)
from sklearn.preprocessing import StandardScaler
import numpy as np

X = np.array([[0], [2], [3], [4], [10]])
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("Standardized Data:\n", X_scaled)

"""*   Applies standardization to scale features using Scikit-learn.
*   Transforms data to have a mean of 0 and standard deviation of 1.
*   Helps improve performance for algorithms sensitive to feature scales.
"""

# 3. One-Hot Encoding
from sklearn.preprocessing import OneHotEncoder
import numpy as np

X_cat = np.array([['red'], ['blue'], ['green']])
ohe = OneHotEncoder(sparse_output=False)
X_ohe = ohe.fit_transform(X_cat)
print("One-Hot Encoded Data:\n", X_ohe)

"""*   Converts categorical data into a numerical format.
*   Creates binary columns for each category to avoid artificial order.
*   Essential for models that require numerical input.
"""