# -*- coding: utf-8 -*-
"""14th Oct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VmP0QElm0cDAqAYbDesO6UsnAukgpxcz

# Polynomial Regression

### 1. Polynomial Regression with Varying Degrees
This example shows how changing the degree of the polynomial affects the model fit.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split

# Generate some non-linear data
np.random.seed(0)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Degrees to experiment with
degrees = [2,10]

plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, label='Training data')
plt.scatter(X_test, y_test, label='Test data')

X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)

for degree in degrees:
    # Create a pipeline
    model = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False),
                          LinearRegression())
    model.fit(X_train, y_train)
    y_plot = model.predict(X_plot)
    plt.plot(X_plot, y_plot, label=f'Degree {degree}')

plt.title('Polynomial Regression with Different Degrees')
plt.xlabel('X')
plt.ylabel('y')
plt.ylim(-5, 15)
plt.legend()
plt.grid(True)
plt.show()

"""### Understanding Mean Squared Error (MSE)

Mean Squared Error (MSE) is a common metric used to evaluate the performance of regression models. It quantifies the average squared difference between the predicted values from the model and the actual observed values.

Here's a breakdown of what MSE represents and how it relates to error:

1.  **Error:** In the context of regression, the error for a single data point is the difference between the actual value ($y_i$) and the value predicted by the model ($\hat{y}_i$). This is often called the residual: $e_i = y_i - \hat{y}_i$.

2.  **Squaring the Error:** The errors are squared ($e_i^2 = (y_i - \hat{y}_i)^2$). Squaring serves two main purposes:
    *   It makes all errors positive, so positive and negative errors don't cancel each other out.
    *   It penalizes larger errors more heavily than smaller ones. This means that outliers or predictions that are far off from the actual values have a significant impact on the MSE.

3.  **Mean:** The squared errors for all data points in the dataset are summed up and then divided by the number of data points ($n$) to get the average.

The formula for MSE is:

$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$

**How MSE Relates to Model Error and Performance:**

*   **Measure of Model Fit:** A lower MSE indicates that the model's predictions are closer to the actual values on average. Therefore, a lower MSE generally suggests a better-fitting model.
*   **Sensitivity to Outliers:** Due to the squaring of errors, MSE is sensitive to outliers. A few predictions that are far from the actual values can significantly increase the MSE.
*   **Units:** The units of MSE are the square of the units of the target variable. This can sometimes make it less intuitive to interpret compared to the Root Mean Squared Error (RMSE), which is the square root of MSE and has the same units as the target variable.
*   **Overfitting and Underfitting:** As seen in the code examples, MSE can help identify overfitting and underfitting.
    *   In **overfitting**, the model fits the training data very well (low training MSE) but performs poorly on unseen test data (high test MSE).
    *   In **underfitting**, the model doesn't fit the training data or the test data well (high training and test MSE).

In summary, MSE provides a single number that summarizes the overall prediction error of a regression model, with larger errors being penalized more heavily. It's a crucial metric for evaluating and comparing different regression models.

### 2. Illustrating Overfitting
This example specifically highlights how a high-degree polynomial model can overfit the training data.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate some non-linear data
np.random.seed(0)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)

# Split data - smaller training set to make overfitting more apparent
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)

# High degree polynomial
degree = 20

plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, label='Training data')
plt.scatter(X_test, y_test, label='Test data')

# Create and train the model
model = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False),
                      LinearRegression())
model.fit(X_train, y_train)

# Evaluate on training and test sets
y_pred_train = model.predict(X_train)
mse_train = mean_squared_error(y_train, y_pred_train)
y_pred_test = model.predict(X_test)
mse_test = mean_squared_error(y_test, y_pred_test)

print(f"Degree {degree} - Training MSE: {mse_train:.2f}")
print(f"Degree {degree} - Test MSE: {mse_test:.2f}")


# Plot the model's prediction
X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)
y_plot = model.predict(X_plot)
plt.plot(X_plot, y_plot, color='red', label=f'Degree {degree} Model')

plt.title('Illustrating Overfitting (High Degree Polynomial)')
plt.xlabel('X')
plt.ylabel('y')
plt.ylim(-5, 15)
plt.legend()
plt.grid(True)
plt.show()

"""Low mse for train & high mse for test = Overfitting

 High MSE for train & high MSE for test = Underfitting

Low MSE for train & Low MSE for test = Perfect fit (low means tending to 0 or as near to 0 as possible)

### 3. Illustrating Underfitting
This example shows how a low-degree polynomial model can underfit the data.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate some non-linear data
np.random.seed(0)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Low degree polynomial (linear model)
degree = 1

plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, label='Training data')
plt.scatter(X_test, y_test, label='Test data')

# Create and train the model
model = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False),
                      LinearRegression())
model.fit(X_train, y_train)

# Evaluate on training and test sets
y_pred_train = model.predict(X_train)
mse_train = mean_squared_error(y_train, y_pred_train)
y_pred_test = model.predict(X_test)
mse_test = mean_squared_error(y_test, y_pred_test)

print(f"Degree {degree} - Training MSE: {mse_train:.2f}")
print(f"Degree {degree} - Test MSE: {mse_test:.2f}")

# Plot the model's prediction
X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)
y_plot = model.predict(X_plot)
plt.plot(X_plot, y_plot, color='red', label=f'Degree {degree} Model')

plt.title('Illustrating Underfitting (Low Degree Polynomial)')
plt.xlabel('X')
plt.ylabel('y')
plt.ylim(-5, 15)
plt.legend()
plt.grid(True)
plt.show()

"""### 4. Using a Validation Set for Hyperparameter Tuning
This example demonstrates using a validation set to select the best polynomial degree to avoid overfitting and underfitting.

**By iterating through different degrees and evaluating their performance on the validation set, the code is performing hyperparameter tuning to find the optimal complexity for the model that generalizes well to unseen data.**
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate some non-linear data
np.random.seed(0)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)

# Split data into training, validation, and testing sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=0) # 0.25 * 0.8 = 0.2

# Degrees to evaluate
degrees_to_try = range(1, 41)
train_errors = []
val_errors = []

for degree in degrees_to_try:
    # Create and train the model on the training set
    model = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False),
                          LinearRegression())
    model.fit(X_train, y_train)

    # Evaluate on training and validation sets
    y_pred_train = model.predict(X_train)
    mse_train = mean_squared_error(y_train, y_pred_train)
    train_errors.append(mse_train)

    y_pred_val = model.predict(X_val)
    mse_val = mean_squared_error(y_val, y_pred_val)
    val_errors.append(mse_val)

# Find the degree with the minimum validation error
best_degree = degrees_to_try[np.argmin(val_errors)]
print(f"Best polynomial degree based on validation error: {best_degree}")

# Train the final model on the combined training and validation data (or just train data)
# and evaluate on the test set for final performance
final_model = make_pipeline(PolynomialFeatures(degree=best_degree, include_bias=False),
                            LinearRegression())
final_model.fit(X_train_val, y_train_val) # Using train + validation for final model
y_pred_test = final_model.predict(X_test)
mse_test = mean_squared_error(y_test, y_pred_test)
print(f"Final Test MSE with best degree ({best_degree}): {mse_test:.2f}")

# Visualize the training and validation errors vs. polynomial degree
plt.figure(figsize=(10, 6))
plt.plot(degrees_to_try, train_errors, label='Training Error')
plt.plot(degrees_to_try, val_errors, label='Validation Error')
plt.axvline(x=best_degree, color='red', linestyle='--', label=f'Best Degree ({best_degree})')
plt.title('Training and Validation Error vs. Polynomial Degree')
plt.xlabel('Polynomial Degree')
plt.ylabel('Mean Squared Error')
plt.yscale('log') # Use log scale for better visibility of errors
plt.legend()
plt.grid(True)
plt.show()

"""# Cross-Validation and Generalization.

1. Holdout Validation
This example shows a simple holdout validation split using `train_test_split`.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Generate some sample data
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# Split data into training and testing sets (holdout)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a simple linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Evaluate the model on the test set
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)

print(f"Holdout Validation - Mean Squared Error on Test Set: {mse:.2f}")

# Visualize the holdout split and the model's prediction
plt.figure(figsize=(8, 6))
plt.scatter(X_train, y_train, label='Training data')
plt.scatter(X_test, y_test, color='red', label='Test data')
plt.plot(X_test, y_pred, color='green', linewidth=2, label='Model prediction')
plt.title('Holdout Validation Split and Model Prediction')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()

"""### 2. K-Fold Cross-Validation
This example demonstrates K-Fold Cross-Validation using `cross_val_score`.
"""

from sklearn.model_selection import cross_val_score, KFold
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# Define the model and number of folds
model = LinearRegression()
kf = KFold(n_splits=5, shuffle=True, random_state=42)
#100% / 5 = 20%, so 20% data goes to testing while 80% goes for training
#100% / 10 = 10%, so 10% data goes to testing while 90% goes for training
#100% / 6 = 16.66% for testing and 83.33 for training.
# Perform cross-validation and get scores for each fold

scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')

# Convert negative MSE to positive
mse_scores = -scores

print(f"K-Fold Cross-Validation - Mean MSE across folds: {mse_scores.mean():.2f}")
print(f"K-Fold Cross-Validation - Standard deviation of MSE across folds: {mse_scores.std():.2f}")

print()
# Find the minimum MSE among the scores from each fold
min_mse = np.min(mse_scores)
print(f"K-Fold Cross-Validation - Minimum MSE across folds: {min_mse:.2f}")

# Visualize the distribution of MSE scores across folds
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(mse_scores) + 1), mse_scores)
plt.xlabel('Fold')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error for Each Fold in K-Fold Cross-Validation')
plt.xticks(range(1, len(mse_scores) + 1))
plt.grid(axis='y')
plt.show()

# Conceptual visualization of K-Fold splits (showing first few folds)
plt.figure(figsize=(10, 2))
y_indices = np.zeros(len(y))
fold_colors = plt.cm.viridis(np.linspace(0, 1, kf.n_splits))

for i, (train_index, test_index) in enumerate(kf.split(X, y)):
    if i < 3: # Show only the first 3 folds for clarity
        y_indices[train_index] = i + 1
        y_indices[test_index] = -(i + 1) # Represent test set with negative values

        plt.scatter(train_index, y_indices[train_index], color=fold_colors[i], marker='_', linewidth=5, label=f'Fold {i+1} Train')
        plt.scatter(test_index, y_indices[test_index], color=fold_colors[i], marker='|', linewidth=5, label=f'Fold {i+1} Test')


plt.title('Conceptual K-Fold Splits (First 3 Folds)')
plt.xlabel('Sample Index')
plt.yticks([]) # Hide y-axis ticks
plt.legend(loc='upper right')
plt.show()

"""### 3. Stratified K-Fold Cross-Validation
This example shows Stratified K-Fold Cross-Validation, useful for classification tasks to maintain class distribution in each fold.
"""

from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Generate some sample classification data
X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)

# Define the model and number of folds (stratified)
model = LogisticRegression()
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform stratified cross-validation and get scores for each fold
scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')

print(f"Stratified K-Fold Cross-Validation - Mean Accuracy across folds: {scores.mean():.2f}")
print(f"Stratified K-Fold Cross-Validation - Standard deviation of Accuracy across folds: {scores.std():.2f}")

# Visualize the distribution of Accuracy scores across folds
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(scores) + 1), scores)
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Accuracy for Each Fold in Stratified K-Fold Cross-Validation')
plt.xticks(range(1, len(scores) + 1))
plt.grid(axis='y')
plt.ylim(0, 1) # Accuracy is between 0 and 1
plt.show()

# Visualize the class distribution in the original data and the first fold (conceptually)
plt.figure(figsize=(10, 4))

# Original data distribution
unique, counts = np.unique(y, return_counts=True)
plt.subplot(1, 2, 1)
plt.bar(unique, counts, tick_label=unique)
plt.title('Original Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(unique)

# First fold distribution (conceptual - based on skf split)
for i, (train_index, test_index) in enumerate(skf.split(X, y)):
    if i == 0:
        _, train_counts = np.unique(y[train_index], return_counts=True)
        _, test_counts = np.unique(y[test_index], return_counts=True)

        plt.subplot(1, 2, 2)
        bar_width = 0.35
        r1 = np.arange(len(unique))
        r2 = [x + bar_width for x in r1]

        plt.bar(r1, train_counts, color='skyblue', width=bar_width, edgecolor='grey', label='Train Fold 1')
        plt.bar(r2, test_counts, color='lightcoral', width=bar_width, edgecolor='grey', label='Test Fold 1')

        plt.title('Class Distribution in First Fold (Train/Test)')
        plt.xlabel('Class')
        plt.ylabel('Count')
        plt.xticks([r + bar_width/2 for r in range(len(unique))], unique)
        plt.legend()
        break # Only show the first fold

plt.tight_layout()
plt.show()

"""Notice how the proportions of the classes in the training and test sets of this fold are similar to the original class distribution. This visually confirms that Stratified K-Fold successfully preserves the class balance in each split."""

# Print accuracy for each fold
for i, accuracy in enumerate(scores):
    print(f"Fold {i+1} Accuracy: {accuracy:.2f}")

# Conceptual visualization of Stratified K-Fold splits (showing first few folds)
plt.figure(figsize=(10, 2))
y_indices = np.zeros(len(y))
fold_colors = plt.cm.viridis(np.linspace(0, 1, skf.n_splits))

for i, (train_index, test_index) in enumerate(skf.split(X, y)):
    if i < 3: # Show only the first 3 folds for clarity
        y_indices[train_index] = i + 1
        y_indices[test_index] = -(i + 1) # Represent test set with negative values

        plt.scatter(train_index, y_indices[train_index], color=fold_colors[i], marker='_', linewidth=5, label=f'Fold {i+1} Train')
        plt.scatter(test_index, y_indices[test_index], color=fold_colors[i], marker='|', linewidth=5, label=f'Fold {i+1} Test')


plt.title('Conceptual Stratified K-Fold Splits (First 3 Folds)')
plt.xlabel('Sample Index')
plt.yticks([]) # Hide y-axis ticks
plt.legend(loc='upper right')
plt.show()

"""### 4. Illustrating Bias-Variance Trade-off (Conceptual)
This example conceptually shows how different model complexities can lead to high bias (underfitting) or high variance (overfitting). It uses polynomial features to create models of different complexities.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error

# Generate some non-linear data
np.random.seed(42)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit models with different degrees (complexities)
degrees = [1, 2, 15] # Linear, Quadratic, High Degree

plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, label='Training data')
plt.scatter(X_test, y_test, label='Test data')

for degree in degrees:
    # Create a pipeline with polynomial features and linear regression
    model = make_pipeline(PolynomialFeatures(degree=degree, include_bias=False),
                          LinearRegression())
    model.fit(X_train, y_train)
    y_pred_test = model.predict(X_test)
    mse_test = mean_squared_error(y_test, y_pred_test)

    # Plot the model's prediction
    X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)
    y_plot = model.predict(X_plot)
    plt.plot(X_plot, y_plot, label=f'Degree {degree} (Test MSE: {mse_test:.2f})')

plt.title('Bias-Variance Trade-off with Polynomial Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.ylim(-5, 15)
plt.legend()
plt.grid(True)
plt.show()

print("\nConceptual Illustration of Bias-Variance Trade-off:")
print("Degree 1 (Underfitting - High Bias): The model is too simple and doesn't capture the underlying pattern.")
print("Degree 2 (Good Fit): The model captures the pattern well.")
print("Degree 10 (Overfitting - High Variance): The model is too complex, fits the training data perfectly but performs poorly on unseen test data.")

"""# USD - INR PREDICTION MODEL
Perform k-fold cross-validation on the USD to INR exchange rate data ("USD_INR_Exchange_Rate_Annual.csv") using a Linear Regression model to find the best set of data to train it on for the most accurate prediction for a given year. Analyze and visualize the results.

Redo the entire code to create a linear regression model using sklearn to predict USD values for a given year using the data from "/content/EXINUS.csv". Preprocess the data https://fred.stlouisfed.org/series/EXINUS

## Load data

### Subtask:
Load the USD to INR exchange rate data from the provided CSV file into a pandas DataFrame.

**Reasoning**:
The first step is to load the data from the CSV file into a pandas DataFrame to begin the analysis as requested by the subtask instructions. Displaying the head and info will help verify the data was loaded correctly and understand its structure.
"""

import pandas as pd

df = pd.read_csv('EXINUS.csv')
display(df.head())
display(df.info())

"""**Reasoning**:
The previous command failed because the file was not found. I need to check the available files to see if the file exists or if the path is incorrect.


"""

import os
print(os.listdir())

"""**Reasoning**:
The output of the previous command shows that the file name is `EXINUS.csv` and not `USD_INR_Exchange_Rate_Annual.csv`. I will now try to load the data using the correct file name.


"""

df = pd.read_csv('EXINUS.csv')
display(df.head())
display(df.info())

"""## Prepare data

### Subtask:
Select the 'Year' as the feature (X) and 'EXINUS' as the target (y).

**Reasoning**:
Convert 'observation_date' to datetime, extract the year, and select the feature and target variables, reshaping the feature variable.
"""

df['observation_date'] = pd.to_datetime(df['observation_date'])
df['Year'] = df['observation_date'].dt.year
X = df[['Year']].values
y = df['EXINUS'].values
display(X[:5])
display(y[:5])

"""## Define model

### Subtask:
Define the Linear Regression model.

**Reasoning**:
Import the LinearRegression class and instantiate a LinearRegression object.
"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()

"""## Perform k-fold cross-validation

### Subtask:
Apply K-Fold cross-validation to the data and model, using an appropriate scoring metric (like negative mean squared error to evaluate prediction accuracy).

**Reasoning**:
Apply K-Fold cross-validation to the data and model using negative mean squared error as the scoring metric.
"""

from sklearn.model_selection import KFold, cross_val_score
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')

"""## Analyze results

### Subtask:
Print and analyze the mean and standard deviation of the cross-validation scores to understand the model's performance consistency.

**Reasoning**:
Calculate and print the mean and standard deviation of the cross-validation scores.
"""

import numpy as np

mse_scores = -scores

print(f"K-Fold Cross-Validation - Mean MSE across folds: {mse_scores.mean():.2f}")
print(f"K-Fold Cross-Validation - Standard deviation of MSE across folds: {mse_scores.std():.2f}")

"""## Visualize results (optional)

### Subtask:
Create visualizations to show the distribution of scores across different folds.

**Reasoning**:
Generate a bar plot to visualize the MSE scores for each fold to understand the distribution of errors across the different splits.
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.bar(range(1, len(mse_scores) + 1), mse_scores)
plt.xlabel('Fold')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error for Each Fold in K-Fold Cross-Validation')
plt.xticks(range(1, len(mse_scores) + 1))
plt.grid(axis='y')
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The mean Mean Squared Error (MSE) across the 5 folds of the K-Fold cross-validation is approximately $27.87$.
*   The standard deviation of the MSE across the folds is approximately $3.48$, indicating a moderate level of variability in the model's performance on different subsets of the data.
*   The bar chart visually confirms the variation in MSE scores across the five folds, with some folds exhibiting higher error than others.

### Insights or Next Steps

*   The moderate standard deviation of the MSE suggests that the model's performance is somewhat sensitive to the specific data split used for training and testing. This could indicate the presence of outliers or periods of higher volatility in the exchange rate data.
*   Further analysis could involve examining the specific folds with higher MSE to understand the characteristics of the data in those splits and potentially identify areas for model improvement or data preprocessing.

"""