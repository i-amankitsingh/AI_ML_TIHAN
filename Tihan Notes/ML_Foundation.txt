Machine Learning Foundations: Linearity, Non-linearity & Outliers
Prerequisites:

Python basics (variables, loops, print())

NumPy arrays & Pandas DataFrames

Basic regression (linear, logistic)

Ability to run Jupyter/Google Colab

1) Definition
This session introduced how models fit data, and why understanding linearity vs. non-linearity is critical. It explained:

How digital apps use ML (backend logic like Uber pricing)

Why NumPy sits at the center of the Python data ecosystem

Difference between sequential & parallel data processing

How outliers distort results

How to interpret parameters (bias & weights) in regression

2) Why it matters
Choosing the wrong model (linear for non-linear data) â†’ poor predictions

Outliers skew mean, regression lines, and evaluation metrics

Real-world data rarely behaves perfectly linear

Understanding data nature = foundation for model selection

Generalization ensures model works on new data, not just training data

3) Core Concepts
a) High-Level Application Architecture
Frontend â†’ User interface (app/web screen)

Backend â†’ Server logic (ML models, APIs)

Database â†’ Storage of users, transactions, logs

Business Logic â†’ Determines how decisions are made

Example:
In ride-hailing apps like Uber/Ola:

# Backend receives request input_data = {"from": "A", "to": "B", "time": "peak"}
predicted_price = ml_model.predict(input_data)
Instead of fixed â‚¹30/km, ML adjusts for time, demand, location.

b) Libraries & Tools
NumPy = Core numerical engine

Arrays, linear algebra, broadcasting
Built on NumPy:

Pandas â†’ Data manipulation

Matplotlib â†’ Basic plotting

Seaborn â†’ Simplified statistical plots

SKLearn â†’ Machine learning

TensorFlow â†’ Deep learning

import numpy as np
arr = np.array([1,2,3,4])
print(arr**2)  # [1, 4, 9, 16]
import pandas as pd, matplotlib.pyplot as plt
data = pd.Series([1,3,2,4])
data.plot(kind="bar"); plt.show()
import seaborn as sns
tips = sns.load_dataset("tips")
sns.boxplot(x="day", y="total_bill", data=tips)
plt.show()
c) Sequential vs Parallel Processing
Pandas â†’ sequential (one step after another)

Spark â†’ parallel (distributes tasks across multiple CPUs)

# Pandas (slow on 50M rows)
df = pd.read_csv("big.csv")
print(df["col"].mean())

# Spark (fast, parallel)
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("example").getOrCreate()
df_spark = spark.read.csv("big.csv", header=True, inferSchema=True)
print(df_spark.selectExpr("avg(col)").collect())
ðŸ’¡ _Rule of Thumb:_ Use Spark when dataset > **5 million rows**.
d) Linearity & Non-linearity
Linear relationship:

y = Î± + Î²x

where Î± = intercept (bias) and Î² = slope (weight)

Non-linear: Relationship curves. E.g., income â†‘ â†’ spending â†‘ until a point â†’ then declines (savings).

import numpy as np, matplotlib.pyplot as plt
x = np.linspace(0,10,100)
y_linear = 2*x + 3
y_nonlinear = -0.5*(x-5)**2 + 20
plt.plot(x, y_linear, label="Linear")
plt.plot(x, y_nonlinear, label="Non-linear")
plt.legend(); plt.show()
Linearity & Non-linearity

e) Model Fitting & Generalization
Analogy: Tailor fitting T-shirts

Tailor uses medium measurements with margin â†’ fits most people

ML model must generalize, not memorize every data point

f) Parameters
Intercept (Î± / bias) = minimum value even when x=0

Coefficient (Î² / weight) = effect of x on y

Notations:

Stats: y = mx + c

ML: y = Î²x + Î±

Deep Learning: y = WÂ·X + b

g) Outliers vs Distribution
Distribution = majority pattern

Outliers = rare, extreme values

Example: Student scores ~60â€“70, but one scores 99 (high outlier), another 35 (low outlier).

import seaborn as sns, pandas as pd, matplotlib.pyplot as plt
scores = pd.DataFrame({"marks":[60,62,70,65,61,95,55,58,54,100]})
sns.boxplot(x=scores["marks"]); plt.show()
download (2)

h) Impact on Statistics
Mean gets pulled by outliers

Median resists outliers

import numpy as np
data = [30,32,31,29,28,1000]  # 1000 = outlier
print("Mean:", np.mean(data))
print("Median:", np.median(data))
i) Categorical Variable Encoding
import pandas as pd 
from sklearn.linear_model import LinearRegression

data = pd.DataFrame({ "bill":[10,20,30], "gender":["Male","Female","Male"]
})
encoded = pd.get_dummies(data, columns=["gender"], drop_first=True)
X = encoded[["bill","gender_Male"]]; y=[1,2,3]

model = LinearRegression().fit(X,y) 
print(model.coef_)
The formula becomes:

y = b + w1 * bill + w2 * gender

j) Model Training & Evaluation
Steps:

Split data (80% train, 20% test)

Train model on training set

Predict test set

Evaluate with metrics

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
# Data
X = np.arange(10).reshape(-1,  1)
y = 2 * X.flatten() + 1
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Model training
model = LinearRegression().fit(X_train, y_train)
# Predictions
y_pred = model.predict(X_test)
# RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE:", rmse)
k) Choosing the Right Model
Linear regression â†’ simple straight-line relation

Polynomial / non-linear â†’ curving data

Neural networks â†’ handle highly complex patterns & noise

Trade-off: Complexity vs explainability

4) Real-World Analogy
Tailor analogy: model fitting = T-shirt sizing

Noise cancellation analogy: Outliers = background noise â†’ model must detect signal vs noise

5) Common Pitfalls & Fixes
Pitfall	What happens	Fix
Use linear model on curved data	Wrong predictions	Check scatter plot before modeling
Remove all outliers	Lose genuine cases	Investigate context
Confuse inconsistency with outlier	Wrong cleaning	E.g. Age=225 = error, not outlier
Use mean on skewed data	Misleading	Use median
6) Practical Applications
Uber/Ola pricing (backend ML logic)

Salary reports â†’ mean vs median salaries mislead

Restaurant tips dataset â†’ shows how bills, tips, and group size affect model fitting

Stock prices â†’ highly non-linear, difficult to model with linear regression

7) Check Yourself
Why is generalization better than overfitting?

Whatâ€™s the difference between an outlier and inconsistent data?

How do Spark and Pandas differ in processing large datasets?

Answers:

Generalization fits majority, works on new data; overfitting memorizes noise.

Outlier = rare but valid; inconsistency = wrong/invalid data.

Pandas = sequential (slow for big data), Spark = parallel (fast for big data).

8) Practice Task
Use the tips dataset (Seaborn):

Build regression model predicting tip from bill + gender + smoker + group size.

Visualize outliers with boxplots.

Run model with & without outliers â†’ compare RMSE.

Hint: Use sns.load_dataset("tips").

9) TL;DR
Linearity check is critical â†’ wrong assumption breaks models

Outliers â‰  errors â†’ investigate before removal

Generalization > memorization

NumPy = core, Pandas/Seaborn/SKLearn built on it

Big data â†’ Spark; small data â†’ Pandas

Quick Reference: ML Terms
Linearity â€” A straight-line relationship between input (x) and output (y).

Non-linearity â€” Any curved/threshold relationship that a straight line canâ€™t capture.

Regression â€” Predicting a numeric value (e.g., price) from one or more inputs.

Linear Regression â€” Regression that fits a straight line/plane to the data.

Polynomial Regression â€” Regression that uses curved terms (e.g., xÂ², xÂ³) to fit bends.

Logistic Regression â€” A classifier that predicts probabilities for two classes (0/1).

Feature (X) â€” An input variable used by the model (e.g., bill amount, BMI).

Target (y) â€” The value the model tries to predict (e.g., tip amount).

Intercept / Bias (Î±, b) â€” The modelâ€™s baseline value when all inputs are zero.

Coefficient / Weight (Î², W) â€” How much the prediction changes when an input rises by one unit.

Generalization â€” The modelâ€™s ability to perform well on new, unseen data.

Overfitting â€” When a model memorizes noise in training data and fails on new data.

Underfitting â€” When a model is too simple and misses real patterns in data.

Distribution â€” How values are spread (typical range vs. rare extremes).

Outlier â€” A data point far from the rest (unusual but not always wrong).

Inconsistent Data â€” Impossible or erroneous values (e.g., age = 225).

Mean (Average) â€” Sum divided by count; pulled by outliers.

Median â€” Middle value; robust to outliers.

Residual â€” Error for one example: actual âˆ’ predicted.

MSE / RMSE â€” Mean (Root Mean) Squared Error; common regression error metrics.

RÂ² (R-squared) â€” Fraction of target variance explained by the model (0â€“1).

Trainâ€“Test Split â€” Divide data to train the model (train) and check it (test).

Validation Set â€” Optional third split used for model/parameter tuning.

One-Hot Encoding â€” Turn categories into 0/1 columns (Maleâ†’[1,0], Femaleâ†’[0,1]).

Dummy Variable â€” A 0/1 column representing one category (from one-hot encoding).

Categorical Variable â€” Text/category input (e.g., day = Mon/Tue/â€¦); needs encoding.

Standardization â€” Rescale a variable to mean 0 and std-dev 1.

Polynomial Features â€” Extra features like xÂ², xÂ³ to capture curvature.

Broadcasting (NumPy) â€” Automatic expansion of arrays to align shapes in operations.

Sequential Processing (Pandas) â€” Work happens step-by-step on one machine.

Parallel / Distributed Processing (Spark) â€” Work split across many machines/cores.

API â€” A programmatic interface that lets apps call your model (e.g., to get a price).

Pipeline â€” A chain of steps: clean data â†’ train â†’ evaluate â†’ deploy.

Matplotlib â€” Base plotting library in Python.

Seaborn â€” High-level plotting library built on Matplotlib (quicker stats visuals).

scikit-learn (SKLearn) â€” Standard Python library for classic ML.

NumPy â€” Core numeric library; arrays and fast math for everything else.

Pandas â€” Tables (DataFrames) for cleaning/joining/filtering data.

TensorFlow â€” Deep learning framework for neural networks.

Histogram â€” Bars showing how often values fall into ranges.

Boxplot â€” A summary of distribution (median, quartiles, outliers).

Scatter Plot â€” Dots showing the relationship between two variables.

KDE (Kernel Density Estimate) â€” A smooth curve showing data density.

Thank You
Thanks for learning with us!
Keep experimenting with small datasets, plot before you model, and prefer clarity over complexity.
Questions or doubts? Note them down right awayâ€”curiosity compounds just like interest.