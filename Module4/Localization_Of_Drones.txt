NOTES: Localization of Drones
How Drones Know Where They Are (GPS, odometry, and sensor fusion)
1. Introduction to Drone Localization
My image My image

1.1 What Is Localization?
Imagine you're blindfolded in a large room. Someone spins you around and asks "where are you?" That's essentially what a drone faces every millisecond it's flying!

Localization is how a drone answers three questions:

Where am I? (position in 3D)
Which way am I facing? (orientation)
How fast am I moving? (velocity)
The drone's state vector captures all this information:

state
(
t
)
=
(
x
,
y
,
z
,
v
x
,
v
y
,
v
z
,
yaw
,
pitch
,
roll
)
state(t)=(x,y,z,v 
x
â€‹
 ,v 
y
â€‹
 ,v 
z
â€‹
 ,yaw,pitch,roll)

Here's the catch: the drone never knows these values perfectly. Its sensors are like friends giving you hintsâ€”some are reliable but slow, others are fast but get confused easily. The drone must constantly piece together these imperfect clues:

x
k
=
true state
+
estimation error
x 
k
â€‹
 =true state+estimation error

Think of it like asking 5 friends where you left your keysâ€”you'd combine their hints and trust the most reliable friend more. That's exactly what drones do!

1.2 Why Localization Matters
A drone's software works in layers:

Sensors â†’ Localization â†’ Planning â†’ Control â†’ Mission

If localization fails, everything breaks down. Imagine GPS saying you're 10m away from where you actually areâ€”the planner sends you the wrong way, and your drone might crash!

Key insight: Every layer above localization depends on getting an accurate state estimate. Poor localization = mission failure.

1.3 Three Ways Drones Locate Themselves
1. GPS: The Global Guide

Like asking a satellite "Hey, where am I on Earth?"
Provides: 
p
W
=
[
x
W
,
y
W
,
z
W
]
p 
W
 =[x 
W
 ,y 
W
 ,z 
W
 ] (position in world frame)
Good: Never gets lost in the big picture, absolute reference
Bad: Updates slowly (~1-10 Hz), meter-level accuracy, fails indoors
Best for: Outdoor navigation, waypoint missions
2. Odometry: The Step Counter

Tracks motion by integrating small movements:
x
k
+
1
=
x
k
+
Î”
x
k
x 
k+1
â€‹
 =x 
k
â€‹
 +Î”x 
k
â€‹
 

Sources: IMU (feels acceleration/rotation), cameras (tracks features), LiDAR (maps surroundings)
Good: Super fast (100-1000 Hz), very smooth
Bad: Driftâ€”small errors compound over time like compound interest
Best for: Smooth flight between GPS updates, indoor flight
3. Sensor Fusion: The Smart Combiner

Blends all sensors using algorithms (like Extended Kalman Filter)
Uses two steps:
Predict: Use IMU to estimate next state
Update: Correct with GPS, camera, etc.
x
^
k
+
1
=
prediction from IMU
+
correction from sensors
x
^
  
k+1
â€‹
 =prediction from IMU+correction from sensors

Good: Combines everyone's strengths, robust to failures
Bad: Complex to tune
Best for: Real-world reliable flying
2. Understanding the Drone's "State"
2.1 What's a State?
A drone's state is like its ID card at any moment. Minimum information needed:

Position: 
x
,
y
,
z
x,y,z (where am I in 3D space?)
Velocity: 
v
x
,
v
y
,
v
z
v 
x
â€‹
 ,v 
y
â€‹
 ,v 
z
â€‹
  (how fast am I moving?)
Orientation: yaw, pitch, roll (which way am I facing?)
We can write this compactly as:

x
=
[
x
,
y
,
z
,
v
x
,
v
y
,
v
z
,
yaw
,
pitch
,
roll
]
T
x=[x,y,z,v 
x
â€‹
 ,v 
y
â€‹
 ,v 
z
â€‹
 ,yaw,pitch,roll] 
T
 

In practice, we also track sensor biases (systematic errors in IMU):

x
full
=
[
p
W
,
v
W
,
q
W
B
,
b
a
,
b
Ï‰
]
T
x 
full
â€‹
 =[p 
W
 ,v 
W
 ,q 
WB
 ,b 
a
â€‹
 ,b 
Ï‰
â€‹
 ] 
T
 

where:

p
W
p 
W
  = position (3 values)
v
W
v 
W
  = velocity (3 values)
q
W
B
q 
WB
  = orientation quaternion (4 values)
b
a
b 
a
â€‹
  = accelerometer bias (3 values)
b
Ï‰
b 
Ï‰
â€‹
  = gyroscope bias (3 values)
Total: 16 state variables that the filter estimates!

2.2 Two Important Coordinate Systems
World Frame 
W
W: Fixed map (like GPS coordinates)

Axes: East, North, Up (ENU convention)
Planning happens here: "fly to (10, 15, 20)"
Body Frame 
B
B: Attached to drone

Axes: Forward, Right, Down (relative to drone)
Sensors read here: "accelerating 2m/sÂ² forward"
The conversion magic: Rotation matrix 
R
B
W
R 
B
W
â€‹
  transforms between frames:

v
W
=
R
B
W
â‹…
v
B
v 
W
 =R 
B
W
â€‹
 â‹…v 
B
 

This says: "a vector in body frame, when transformed by 
R
B
W
R 
B
W
â€‹
 , gives its equivalent in world frame."

Example: If IMU says "2 m/sÂ² forward" and drone is tilted 45Â° up:

a
W
=
R
B
W
[
2
0
0
]
â‰ˆ
[
1.41
0
1.41
]
a 
W
 =R 
B
W
â€‹
  
â€‹
  
2
0
0
â€‹
  
â€‹
 â‰ˆ 
â€‹
  
1.41
0
1.41
â€‹
  
â€‹
 

The forward acceleration now has components in both horizontal and vertical directions!

3. The Sensor Team
Think of sensors as a team where everyone has a specialty:

3.1 GPS: The Slow but Wise Elder
GPS talks to satellites to figure out position. After coordinate conversion to local frame:

z
k
GPS
=
[
x
W
y
W
z
W
]
+
v
GPS
z 
k
GPS
â€‹
 = 
â€‹
  
x 
W
 
y 
W
 
z 
W
 
â€‹
  
â€‹
 +v 
GPS
 

where 
v
GPS
v 
GPS
  is measurement noise (typically 2-5m standard deviation).

Measurement model: GPS directly observes position components:

z
GPS
=
H
GPS
â‹…
x
+
v
GPS
z 
GPS
 =H 
GPS
â€‹
 â‹…x+v 
GPS
 

where 
H
GPS
=
[
I
3
0
0
â‹¯
â€‰
]
H 
GPS
â€‹
 =[I 
3
â€‹
 00â‹¯] just picks out the position part from full state.

Personality traits:

âœ“ Reliable outdoors
âœ— Slow updates (1-10 Hz)
âœ— Goes blind indoors
âœ— No orientation info
3.2 IMU: The Hyperactive Youngster
The IMU has two sensors providing the motion model backbone:

Gyroscope measures angular rate: 
Ï‰
meas
B
=
Ï‰
true
B
+
b
Ï‰
+
n
Ï‰
Ï‰ 
meas
B
â€‹
 =Ï‰ 
true
B
â€‹
 +b 
Ï‰
â€‹
 +n 
Ï‰
â€‹
 

After bias correction 
Ï‰
~
k
B
=
Ï‰
meas
B
âˆ’
b
^
Ï‰
Ï‰
~
  
k
B
â€‹
 =Ï‰ 
meas
B
â€‹
 âˆ’ 
b
^
  
Ï‰
â€‹
 , integrated to update orientation over time 
Î”
t
Î”t: 
R
k
+
1
W
B
=
R
k
W
B
â‹…
exp
â¡
(
[
Ï‰
~
k
B
Î”
t
]
Ã—
)
R 
k+1
W
â€‹
  
B
â€‹
 =R 
k
W
â€‹
  
B
â€‹
 â‹…exp([ 
Ï‰
~
  
k
B
â€‹
 Î”t] 
Ã—
â€‹
 )

Accelerometer measures specific force: 
f
meas
B
=
R
W
B
(
a
W
âˆ’
g
W
)
+
b
a
+
n
a
f 
meas
B
â€‹
 =R 
W
B
â€‹
 (a 
W
 âˆ’g 
W
 )+b 
a
â€‹
 +n 
a
â€‹
 

After bias correction 
f
~
k
B
=
f
meas
B
âˆ’
b
^
a
f
~
â€‹
  
k
B
â€‹
 =f 
meas
B
â€‹
 âˆ’ 
b
^
  
a
â€‹
  and transforming to world frame: 
a
W
=
R
B
W
(
f
~
k
B
)
+
g
W
a 
W
 =R 
B
W
â€‹
 ( 
f
~
â€‹
  
k
B
â€‹
 )+g 
W
 

Then integrate to get velocity and position: 
v
k
+
1
W
=
v
k
W
+
a
k
W
Î”
t
v 
k+1
W
â€‹
 =v 
k
W
â€‹
 +a 
k
W
â€‹
 Î”t 
p
k
+
1
W
=
p
k
W
+
v
k
W
Î”
t
+
1
2
a
k
W
(
Î”
t
)
2
p 
k+1
W
â€‹
 =p 
k
W
â€‹
 +v 
k
W
â€‹
 Î”t+ 
2
1
â€‹
 a 
k
W
â€‹
 (Î”t) 
2
 

The drift problem: A tiny 0.01 m/sÂ² bias leads to:

After 1 sec: velocity error â‰ˆ 0.01 m/s
After 10 sec: position error â‰ˆ 0.5 m
After 60 sec: position error â‰ˆ 18 m! (grows as 
1
2
b
t
2
2
1
â€‹
 bt 
2
 )
Personality traits:

âœ“ Super fast (200-1000 Hz)
âœ“ Very smooth
âœ— Drifts terribly over time
3.3 Cameras: The Visual Detective
Visual Odometry tracks features (corners, edges) across frames to estimate relative motion 
Î”
T
Î”T.

The camera projects 3D world point 
P
W
P 
W
  to 2D pixel 
(
u
,
v
)
(u,v):

[
u
v
1
]
âˆ
K
â‹…
R
W
C
(
P
W
âˆ’
p
W
)
â€‹
  
u
v
1
â€‹
  
â€‹
 âˆKâ‹…R 
W
C
â€‹
 (P 
W
 âˆ’p 
W
 )

where 
K
K is the camera calibration matrix.

By tracking the same feature in frames 
k
k and 
k
+
1
k+1, we estimate:

Î”
T
k
k
+
1
=
[
R
k
k
+
1
t
k
k
+
1
0
1
]
Î”T 
k
k+1
â€‹
 =[ 
R 
k
k+1
â€‹
 
0
â€‹
  
t 
k
k+1
â€‹
 
1
â€‹
 ]

Global pose comes from composing increments: 
T
k
+
N
=
T
k
â‹…
Î”
T
k
k
+
1
â‹…
Î”
T
k
+
1
k
+
2
â‹¯
T 
k+N
â€‹
 =T 
k
â€‹
 â‹…Î”T 
k
k+1
â€‹
 â‹…Î”T 
k+1
k+2
â€‹
 â‹¯

The catch: Each small error multiplies down the chain â†’ drift.

Comes in flavors:

Monocular (one camera): Can't tell scaleâ€”is that a toy car 1m away or real car 10m away?
Stereo/RGB-D (depth camera): Knows distances, provides full metric scale
Personality traits:

âœ“ Works well with texture and light
âœ“ GPS-denied operation
âœ— Struggles with blank walls
âœ— Fails in darkness
3.4 LiDAR: The Geometry Expert
Shoots laser beams and measures distances to create 3D point clouds. Matches consecutive clouds to estimate movement.

A LiDAR scan at time 
k
k is a set of 3D points: 
P
k
=
{
P
i
,
k
L
}
=
{
(
x
i
,
k
L
,
y
i
,
k
L
,
z
i
,
k
L
)
}
P 
k
â€‹
 ={P 
i,k
L
â€‹
 }={(x 
i,k
L
â€‹
 ,y 
i,k
L
â€‹
 ,z 
i,k
L
â€‹
 )}

LiDAR odometry estimates the relative transform 
Î”
T
k
k
+
1
Î”T 
k
k+1
â€‹
  that best aligns two consecutive scans:

P
i
,
k
+
1
L
â‰ˆ
Î”
T
k
k
+
1
P
m
(
i
)
,
k
L
P 
i,k+1
L
â€‹
 â‰ˆÎ”T 
k
k+1
â€‹
 P 
m(i),k
L
â€‹
 

Personality traits:

âœ“ Works in dark
âœ“ Loves 3D structure
âœ— Expensive and heavy
3.5 Supporting Cast
Barometer: Measures altitude via air pressure 
z
baro
=
z
W
+
v
baro
z 
baro
 =z 
W
 +v 
baro
 

Great for holding height, but drifts slowly with weather changes.

Magnetometer: Provides heading by measuring Earth's magnetic field 
z
mag
=
R
W
B
â‹…
B
W
+
v
mag
z 
mag
 =R 
W
B
â€‹
 â‹…B 
W
 +v 
mag
 

Helps with heading but gets confused near metal.

Optical Flow: Downward camera watching ground texture 
z
flow
=
[
v
x
B
v
y
B
]
+
v
flow
z 
flow
 =[ 
v 
x
B
â€‹
 
v 
y
B
â€‹
 
â€‹
 ]+v 
flow
 

Says "ground is moving this fast below me."

4. GPS-Based Localization
4.1 How GPS Works (Simplified)
GPS receiver measures pseudorange (distance) to multiple satellites:

Ï
i
=
âˆ£
p
E
âˆ’
s
i
E
âˆ£
+
c
Î”
t
u
+
Ïµ
i
Ï 
i
â€‹
 =âˆ£p 
E
 âˆ’s 
i
E
â€‹
 âˆ£+cÎ”t 
u
â€‹
 +Ïµ 
i
â€‹
 

where:

p
E
p 
E
  = your position in Earth-centered frame (unknown)
s
i
E
s 
i
E
â€‹
  = satellite 
i
i position (known from broadcast)
c
Î”
t
u
cÎ”t 
u
â€‹
  = clock bias error (speed of light Ã— time error)
Ïµ
i
Ïµ 
i
â€‹
  = noise (multipath, atmosphere, etc.)
The process:

Your drone listens to satellites broadcasting "I'm here, and my clock says it's exactly this time"
Drone measures how long signals took to arrive (time Ã— speed of light = distance)
With 4+ satellites, it solves: "Where could I be such that I'm distance 
Ï
1
Ï 
1
â€‹
  from satellite 1, 
Ï
2
Ï 
2
â€‹
  from satellite 2, etc.?"
Converts answer from "latitude/longitude" to "meters East/North/Up" relative to takeoff point using rotation 
R
E
W
R 
E
W
â€‹
 
4.2 GPS in the Estimation Framework
After conversion to local ENU frame:

z
k
GPS
=
[
x
k
W
y
k
W
z
k
W
]
+
v
k
GPS
z 
k
GPS
â€‹
 = 
â€‹
  
x 
k
W
â€‹
 
y 
k
W
â€‹
 
z 
k
W
â€‹
 
â€‹
  
â€‹
 +v 
k
GPS
â€‹
 

where 
v
k
GPS
âˆ¼
N
(
0
,
R
GPS
)
v 
k
GPS
â€‹
 âˆ¼N(0,R 
GPS
â€‹
 ) with typical 
R
GPS
R 
GPS
â€‹
  having 2-5m standard deviation.

Matrix form in the filter: 
z
k
GPS
=
H
GPS
â€‰
x
k
+
v
k
GPS
z 
k
GPS
â€‹
 =H 
GPS
â€‹
 x 
k
â€‹
 +v 
k
GPS
â€‹
 

where 
H
GPS
=
[
I
3
0
0
â‹¯
â€‰
]
H 
GPS
â€‹
 =[I 
3
â€‹
 00â‹¯] selects position from full state.

4.3 The Reality Check
Errors creep in from:

Signals bouncing off buildings (multipath)
Atmosphere messing with signal speed
Satellite positions not perfectly known
Your receiver's clock being slightly off
Result: typical accuracy is 2-5 meters horizontally, worse vertically.

Key insight: GPS acts as a global anchor. While IMU drifts quadratically, GPS periodically resets the position estimate. The filter balances them based on their respective uncertainties (
R
GPS
R 
GPS
â€‹
  vs prediction covariance).

5. Odometry-Based Localization
5.1 The Core Idea
Odometry is dead reckoning: "I know where I started, and I've been keeping track of every move."

The basic equation: 
x
k
+
1
=
x
k
+
Î”
x
k
x 
k+1
â€‹
 =x 
k
â€‹
 +Î”x 
k
â€‹
 

Sounds great, but here's the problem: The Drift Monster.

5.2 IMU Odometry: Fast but Drifty
The core integration loop runs at 200-1000 Hz:

Step 1: Update orientation from gyroscope

First, bias-correct: 
Ï‰
~
k
B
=
Ï‰
meas
B
âˆ’
b
^
Ï‰
Ï‰
~
  
k
B
â€‹
 =Ï‰ 
meas
B
â€‹
 âˆ’ 
b
^
  
Ï‰
â€‹
 

Then integrate: 
R
k
+
1
=
R
k
â‹…
exp
â¡
(
[
Ï‰
~
k
B
Î”
t
]
Ã—
)
R 
k+1
â€‹
 =R 
k
â€‹
 â‹…exp([ 
Ï‰
~
  
k
B
â€‹
 Î”t] 
Ã—
â€‹
 )

Step 2: Transform accelerometer to world frame

First, bias-correct: 
f
~
k
B
=
f
meas
B
âˆ’
b
^
a
f
~
â€‹
  
k
B
â€‹
 =f 
meas
B
â€‹
 âˆ’ 
b
^
  
a
â€‹
 

Then transform: 
a
k
W
=
R
B
W
(
f
~
k
B
)
+
g
W
a 
k
W
â€‹
 =R 
B
W
â€‹
 ( 
f
~
â€‹
  
k
B
â€‹
 )+g 
W
 

Step 3: Integrate to get velocity and position 
v
k
+
1
W
=
v
k
W
+
a
k
W
Î”
t
v 
k+1
W
â€‹
 =v 
k
W
â€‹
 +a 
k
W
â€‹
 Î”t 
p
k
+
1
W
=
p
k
W
+
v
k
W
Î”
t
+
1
2
a
k
W
(
Î”
t
)
2
p 
k+1
W
â€‹
 =p 
k
W
â€‹
 +v 
k
W
â€‹
 Î”t+ 
2
1
â€‹
 a 
k
W
â€‹
 (Î”t) 
2
 

5.3 Why IMU Drifts
Imagine your accelerometer has a tiny 0.01 m/sÂ² bias (barely noticeable):

After 1 second:

Velocity error: 
Î´
v
â‰ˆ
b
â‹…
t
=
0.01
Ã—
1
=
0.01
Î´vâ‰ˆbâ‹…t=0.01Ã—1=0.01 m/s
Position error: 
Î´
p
â‰ˆ
1
2
b
â‹…
t
2
=
0.5
Ã—
0.01
Ã—
1
2
=
0.005
Î´pâ‰ˆ 
2
1
â€‹
 bâ‹…t 
2
 =0.5Ã—0.01Ã—1 
2
 =0.005 m
After 10 seconds:

Velocity error: 
Î´
v
=
0.1
Î´v=0.1 m/s
Position error: 
Î´
p
=
0.5
Î´p=0.5 m
After 60 seconds:

Velocity error: 
Î´
v
=
0.6
Î´v=0.6 m/s
Position error: 
Î´
p
=
18
Î´p=18 m!
Mathematical form:

Velocity error grows linearly: 
Î´
v
(
t
)
â‰ˆ
b
â‹…
t
Î´v(t)â‰ˆbâ‹…t
Position error grows quadratically: 
Î´
p
(
t
)
â‰ˆ
1
2
b
â‹…
t
2
Î´p(t)â‰ˆ 
2
1
â€‹
 bâ‹…t 
2
 
This is like compound interest, but for errors! Orientation errors also rotate gravity incorrectly, compounding the problem further.

5.4 Visual Odometry: Environment Awareness
VO estimates relative motion by minimizing reprojection error:

min
â¡
R
,
t
âˆ‘
i
âˆ¥
Ï€
(
R
â‹…
P
i
+
t
)
âˆ’
[
u
i
â€²
v
i
â€²
]
âˆ¥
2
min 
R,t
â€‹
 âˆ‘ 
i
â€‹
  
â€‹
 Ï€(Râ‹…P 
i
â€‹
 +t)âˆ’[ 
u 
i
â€²
â€‹
 
v 
i
â€²
â€‹
 
â€‹
 ] 
â€‹
  
2
 

where:

Ï€
(
â‹…
)
Ï€(â‹…) is camera projection function
P
i
P 
i
â€‹
  are 3D points
(
u
i
â€²
,
v
i
â€²
)
(u 
i
â€²
â€‹
 ,v 
i
â€²
â€‹
 ) are observed pixel locations in frame 2
Drift accumulation: Each relative transform 
Î”
T
Î”T has small error. After 
N
N steps:

T
N
=
T
0
âˆ
i
=
0
N
âˆ’
1
Î”
T
i
T 
N
â€‹
 =T 
0
â€‹
 âˆ 
i=0
Nâˆ’1
â€‹
 Î”T 
i
â€‹
 

Errors multiply along the chain, causing drift. Like compounding interest, but for mistakes!

5.5 Why Odometry Matters Despite Drift
Because it's FAST and SMOOTH. Between GPS updates (which come once per second), IMU gives you hundreds of updates, keeping your estimate current. You just need GPS to occasionally reset the drift.

The partnership:

IMU provides shape and smoothness at high frequency
GPS provides anchor and prevents long-term drift
Together they give you the best of both worlds!
6. Sensor Fusion: The Magic Glue
6.1 The Problem Setup
You have multiple "friends" (sensors) giving you hints:

GPS says: "You're at position (5, 3, 2)" but only updates once per second
IMU says: "You accelerated 1m/sÂ² forward" 200 times per second, but drifts
Barometer says: "You're at altitude 2.1m" but has slow drift
Camera says: "You moved 0.3m right, 0.1m up" occasionally when features tracked well
The question: What do you actually believe?

6.2 The Bayesian Framework
We want the belief (probability distribution) about the state, given all past measurements and control inputs. Let's denote:

x
k
x 
k
â€‹
  = state at time 
k
k
z
1
:
k
z 
1:k
â€‹
  = all measurements from time 1 to 
k
k
u
1
:
k
u 
1:k
â€‹
  = all control inputs from time 1 to 
k
k
We seek: probability of state 
x
k
x 
k
â€‹
  given measurements 
z
1
:
k
z 
1:k
â€‹
  and inputs 
u
1
:
k
u 
1:k
â€‹
 

This is updated through two steps:

PREDICT (using motion model):

The prediction step computes: probability of 
x
k
x 
k
â€‹
  given measurements up to 
k
âˆ’
1
kâˆ’1

This involves integrating over all possible previous states, weighted by:

How likely we transition from 
x
k
âˆ’
1
x 
kâˆ’1
â€‹
  to 
x
k
x 
k
â€‹
  (motion model)
Our previous belief about 
x
k
âˆ’
1
x 
kâˆ’1
â€‹
 
Translation: "Use my motion model to predict where I'll be, considering uncertainty in my previous state."

UPDATE (using measurements):

The update step computes: probability of 
x
k
x 
k
â€‹
  given all measurements up to 
k
k

This is proportional to:

How likely we'd measure 
z
k
z 
k
â€‹
  if state were 
x
k
x 
k
â€‹
  (measurement model)
Our predicted belief about 
x
k
x 
k
â€‹
  (from prediction step)
Translation: "Now that I have a new measurement, update my belief by comparing what I predicted vs what I actually saw."

Think of it as: "Predict where you'll be based on motion, then correct based on what sensors actually see."

6.3 Kalman Filter: When Everything is Nice and Linear
For linear systems with Gaussian noise, the math becomes elegant!

System models:

Motion model: 
x
k
+
1
=
F
k
x
k
+
G
k
u
k
+
w
k
x 
k+1
â€‹
 =F 
k
â€‹
 x 
k
â€‹
 +G 
k
â€‹
 u 
k
â€‹
 +w 
k
â€‹
  where 
w
k
âˆ¼
N
(
0
,
Q
k
)
w 
k
â€‹
 âˆ¼N(0,Q 
k
â€‹
 )

Measurement model: 
z
k
=
H
k
x
k
+
v
k
z 
k
â€‹
 =H 
k
â€‹
 x 
k
â€‹
 +v 
k
â€‹
  where 
v
k
âˆ¼
N
(
0
,
R
k
)
v 
k
â€‹
 âˆ¼N(0,R 
k
â€‹
 )

The filter maintains a Gaussian belief: 
x
k
âˆ¼
N
(
x
^
k
âˆ£
k
,
P
k
âˆ£
k
)
x 
k
â€‹
 âˆ¼N( 
x
^
  
kâˆ£k
â€‹
 ,P 
kâˆ£k
â€‹
 )

Prediction step (runs at IMU rate): 
x
^
k
+
1
âˆ£
k
=
F
k
x
^
k
âˆ£
k
+
G
k
u
k
x
^
  
k+1âˆ£k
â€‹
 =F 
k
â€‹
  
x
^
  
kâˆ£k
â€‹
 +G 
k
â€‹
 u 
k
â€‹
  
P
k
+
1
âˆ£
k
=
F
k
P
k
âˆ£
k
F
k
T
+
Q
k
P 
k+1âˆ£k
â€‹
 =F 
k
â€‹
 P 
kâˆ£k
â€‹
 F 
k
T
â€‹
 +Q 
k
â€‹
 

Translation: "Propagate state forward using motion model. Uncertainty grows by 
Q
k
Q 
k
â€‹
 ."

Update step (when measurement arrives):

Innovation (how different is measurement from prediction?): 
y
k
+
1
=
z
k
+
1
âˆ’
H
k
+
1
x
^
k
+
1
âˆ£
k
y 
k+1
â€‹
 =z 
k+1
â€‹
 âˆ’H 
k+1
â€‹
  
x
^
  
k+1âˆ£k
â€‹
 

Kalman gain (how much to trust measurement vs prediction?): 
K
k
+
1
=
P
k
+
1
âˆ£
k
H
k
+
1
T
(
H
k
+
1
P
k
+
1
âˆ£
k
H
k
+
1
T
+
R
k
+
1
)
âˆ’
1
K 
k+1
â€‹
 =P 
k+1âˆ£k
â€‹
 H 
k+1
T
â€‹
 (H 
k+1
â€‹
 P 
k+1âˆ£k
â€‹
 H 
k+1
T
â€‹
 +R 
k+1
â€‹
 ) 
âˆ’1
 

State update (blend prediction with measurement): 
x
^
k
+
1
âˆ£
k
+
1
=
x
^
k
+
1
âˆ£
k
+
K
k
+
1
y
k
+
1
x
^
  
k+1âˆ£k+1
â€‹
 = 
x
^
  
k+1âˆ£k
â€‹
 +K 
k+1
â€‹
 y 
k+1
â€‹
 

Covariance update (uncertainty decreases): 
P
k
+
1
âˆ£
k
+
1
=
(
I
âˆ’
K
k
+
1
H
k
+
1
)
P
k
+
1
âˆ£
k
P 
k+1âˆ£k+1
â€‹
 =(Iâˆ’K 
k+1
â€‹
 H 
k+1
â€‹
 )P 
k+1âˆ£k
â€‹
 

6.4 The Trust Game
The Kalman gain 
K
K is the key to the trust game:

Small 
R
R (trusted sensor):

Sensor noise is small
Innovation covariance 
(
H
P
H
T
+
R
)
(HPH 
T
 +R) is small
K
K becomes large
Strong correction toward measurement
"I trust this sensor, let me move my estimate toward it!"
Large 
R
R (noisy sensor):

Sensor noise is large
Innovation covariance is large
K
K becomes small
Gentle correction
"This sensor is unreliable, I'll mostly stick with my prediction."
Large 
Q
Q (uncertain model):

Prediction uncertainty grows fast
P
k
âˆ£
k
âˆ’
1
P 
kâˆ£kâˆ’1
â€‹
  becomes large
Next update will have larger 
K
K
Trust sensors more
"My model is uncertain, please sensors, guide me!"
This is the mathematical formulation of "combining testimonies from witnesses with different reliability."

6.5 Extended Kalman Filter: When Reality is Nonlinear
Drones are nonlinear! Motion involves rotations on SO(3), gravity projection, quaternion dynamics. Solution: linearize around current estimate.

Nonlinear models: 
x
k
+
1
=
f
(
x
k
,
u
k
)
+
w
k
x 
k+1
â€‹
 =f(x 
k
â€‹
 ,u 
k
â€‹
 )+w 
k
â€‹
  
z
k
=
h
(
x
k
)
+
v
k
z 
k
â€‹
 =h(x 
k
â€‹
 )+v 
k
â€‹
 

where 
f
f and 
h
h are nonlinear functions.

Linearization: Compute Jacobians at current estimate: 
F
k
=
âˆ‚
f
âˆ‚
x
âˆ£
x
^
k
âˆ£
k
,
H
k
=
âˆ‚
h
âˆ‚
x
âˆ£
x
^
k
âˆ£
k
F 
k
â€‹
 = 
âˆ‚x
âˆ‚f
â€‹
  
â€‹
  
x
^
  
kâˆ£k
â€‹
 
â€‹
 ,H 
k
â€‹
 = 
âˆ‚x
âˆ‚h
â€‹
  
â€‹
  
x
^
  
kâˆ£k
â€‹
 
â€‹
 

EKF algorithm:

Nonlinear prediction: 
x
^
k
+
1
âˆ£
k
=
f
(
x
^
k
âˆ£
k
,
u
k
)
x
^
  
k+1âˆ£k
â€‹
 =f( 
x
^
  
kâˆ£k
â€‹
 ,u 
k
â€‹
 )

Linearized covariance: 
P
k
+
1
âˆ£
k
=
F
k
P
k
âˆ£
k
F
k
T
+
Q
k
P 
k+1âˆ£k
â€‹
 =F 
k
â€‹
 P 
kâˆ£k
â€‹
 F 
k
T
â€‹
 +Q 
k
â€‹
 

When measurement arrives:

Predict measurement: 
z
^
k
+
1
=
h
(
x
^
k
+
1
âˆ£
k
)
z
^
  
k+1
â€‹
 =h( 
x
^
  
k+1âˆ£k
â€‹
 )
Innovation: 
y
k
+
1
=
z
k
+
1
âˆ’
z
^
k
+
1
y 
k+1
â€‹
 =z 
k+1
â€‹
 âˆ’ 
z
^
  
k+1
â€‹
 
Use linearized 
H
k
H 
k
â€‹
  in standard Kalman update equations
Typical drone state (16 variables): 
x
=
[
p
W
,
v
W
,
q
W
B
,
b
a
,
b
Ï‰
]
T
x=[p 
W
 ,v 
W
 ,q 
WB
 ,b 
a
â€‹
 ,b 
Ï‰
â€‹
 ] 
T
 

p
W
p 
W
  (3): Position in world frame
v
W
v 
W
  (3): Velocity in world frame
q
W
B
q 
WB
  (4): Orientation quaternion
b
a
b 
a
â€‹
  (3): Accelerometer bias
b
Ï‰
b 
Ï‰
â€‹
  (3): Gyroscope bias
6.6 Multi-Rate Asynchronous Fusion
The real magic: sensors run at different rates!

Timeline example:

t=0ms:    IMU predict
t=5ms:    IMU predict  
t=10ms:   IMU predict + Barometer update
t=15ms:   IMU predict
t=20ms:   IMU predict
t=25ms:   IMU predict
...
t=100ms:  IMU predict + GPS update (big correction!)
t=105ms:  IMU predict
t=110ms:  IMU predict + Visual Odometry update
t=115ms:  IMU predict
...
Implementation strategy:

IMU drives predictions at high rate (200-1000 Hz) â†’ keeps estimate current
Each sensor updates whenever new data arrives (asynchronous)
Updates can be processed sequentially (mathematically equivalent to stacked measurement under independence assumption)
Example measurement models:

GPS: 
z
GPS
=
[
x
W
y
W
z
W
]
+
v
GPS
z 
GPS
 = 
â€‹
  
x 
W
 
y 
W
 
z 
W
 
â€‹
  
â€‹
 +v 
GPS
 

Barometer: 
z
baro
=
z
W
+
v
baro
z 
baro
 =z 
W
 +v 
baro
 

Magnetometer: 
z
mag
=
R
W
B
(
q
)
â‹…
B
W
+
v
mag
z 
mag
 =R 
W
B
â€‹
 (q)â‹…B 
W
 +v 
mag
 

Visual Odometry: 
z
VO
=
v
W
+
v
VO
(velocity form)
z 
VO
 =v 
W
 +v 
VO
 (velocity form)

6.7 The Result: A Beautiful Symphony
You get an estimate that:

âœ“ Updates at IMU rate (hundreds of Hz) â†’ smooth for control loops

âœ“ Stays globally accurate via GPS â†’ no long-term drift

âœ“ Adapts to sensor quality â†’ if GPS degrades, automatically trust vision more

âœ“ Robust to dropouts â†’ if one sensor fails, others compensate

âœ“ Quantifies uncertainty â†’ covariance 
P
P tells you "how confident am I?"

6.8 Covariance Tuning: The Art of Balance
The key to making fusion work is tuning covariances:

Process noise 
Q
Q:

Represents: IMU quality, unmodeled dynamics, bias random walk
Large 
Q
Q â†’ "My model is uncertain, trust measurements more"
Small 
Q
Q â†’ "My model is good, don't overreact to noisy sensors"
GPS noise 
R
GPS
R 
GPS
â€‹
 :

Open sky: Small 
R
R (2-3m std) â†’ strong corrections
Near buildings: Large 
R
R (10m std) â†’ gentle corrections
Complete dropout: Don't update at all
Barometer noise 
R
baro
R 
baro
â€‹
 :

Typically small for relative altitude
Account for slow drift
Vision noise 
R
VO
R 
VO
â€‹
 :

Good features: Small 
R
R â†’ trust it
Poor tracking: Large 
R
R or reject measurement
This tuning is part art, part scienceâ€”learned through flight testing and simulation!

Summary: The Big Picture
Drone localization is like a detective solving "where am I?" using testimonies from unreliable witnesses:

The Witnesses
GPS = The wise elder who knows the big picture but speaks slowly

Measurement: 
z
GPS
=
p
W
+
v
GPS
z 
GPS
 =p 
W
 +v 
GPS
 
Role: Global anchor, prevents long-term drift
Update rate: 1-10 Hz
Weakness: Slow, noisy (2-5m), fails indoors
IMU = The hyperactive youngster with constant updates but faulty memory

Motion model: 
p
k
+
1
=
p
k
+
v
k
Î”
t
+
1
2
a
k
(
Î”
t
)
2
p 
k+1
â€‹
 =p 
k
â€‹
 +v 
k
â€‹
 Î”t+ 
2
1
â€‹
 a 
k
â€‹
 (Î”t) 
2
 
Role: High-rate smooth prediction
Update rate: 200-1000 Hz
Weakness: Drifts quadratically (
âˆ¼
t
2
âˆ¼t 
2
 ) over time
Vision/LiDAR = The observant detective watching the environment

Measurement: Relative motion 
Î”
T
Î”T or velocity 
v
W
v 
W
 
Role: Local corrections, enables GPS-denied operation
Update rate: 10-30 Hz
Weakness: Environment-dependent, also drifts
The Smart Detective (EKF)
Framework: 
x
^
k
+
1
=
f
(
x
^
k
,
u
k
)
âŸ
Predict with IMU
+
K
â‹…
(
z
âˆ’
h
(
x
^
)
)
âŸ
Correct with measurement
x
^
  
k+1
â€‹
 = 
Predict with IMU
f( 
x
^
  
k
â€‹
 ,u 
k
â€‹
 )
â€‹
 
â€‹
 + 
Correct with measurement
Kâ‹…(zâˆ’h( 
x
^
 ))
â€‹
 
â€‹
 

How it works:

Predict: Use IMU to propagate state forward at high rate
Trust: Compute Kalman gain 
K
K based on uncertainties (
P
P, 
Q
Q, 
R
R)
Update: When sensor data arrives, blend prediction with measurement
Adapt: Automatically adjust trust based on sensor quality
Result: Fast, accurate, robust estimate that no single sensor could provide!

The Three Techniques Working Together
GPS (absolute reference)
    â†“ [corrects long-term drift]
    â†“
Sensor Fusion (EKF) â†â†’ [provides smooth high-rate estimate]
    â†‘
    â†‘ [provides short-term motion]
Odometry (IMU/Vision/LiDAR)
GPS prevents long-term drift (absolute reference, but slow) â†• Sensor Fusion intelligently combines everything (optimal blending via Kalman gain) â†• Odometry provides high-rate smooth estimates (relative motion, but drifts)

Why Modern Drones Fly Reliably
Because they:

Don't trust any single sensor â†’ each has weaknesses
Blend imperfect information mathematically â†’ EKF provides optimal fusion
Adapt in real-time â†’ adjust trust based on conditions
Maintain uncertainty â†’ know when they're confident vs uncertain
The bottom line: Turn a collection of noisy, biased, partial observations into a single coherent understanding of "where am I and how am I moving" that's accurate enough to land on a moving target!

This is the magic of sensor fusionâ€”transforming unreliable pieces into something remarkably reliable! ğŸš