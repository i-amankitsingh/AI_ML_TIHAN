Lecture Summary

Overview
The lecture focuses on Regression Model Evaluation, specifically exploring polynomial regression as an extension of simple linear regression and multiple linear regression. It covers the mathematical foundation of polynomial regression, its computational complexity, and the important concepts of bias, variance, the bias-variance trade-off, overfitting, underfitting, and how to choose hyperparameters like polynomial degree. The lecture also introduces training, validation, and testing splits used to evaluate model performance.

1. Polynomial Regression
Motivation: Simple linear regression works well only when there is a linear relationship between input X and output Y. For nonlinear relationships, a linear model cannot fit the data adequately.
Polynomial Regression Model: Extends linear regression by including powers of the input variable, e.g.,
[ Y = B_0 + B_1 X + B_2 X^2 + B_3 X^3 + \ldots + B_d X^d ] where only the weights (B_i) are learned; powers of (X) are predefined based on the data.
Why is polynomial regression still linear? Because the model is linear in terms of the weights (B_i). The nonlinearity lies in the powers of input (X), which are fixed features and not learned. Thus, the model remains a linear combination of features, making optimization easier.
Example: Given (X = 3), powers like (X^2 = 9), (X^3=27) are calculated beforehand. The model learns only the weights (B_0, B_1, B_2, B_3).
2. Complexity and Comparison
Complexity: Polynomial regression has slightly more parameters than simple linear regression (more weights for powers of (X)) but is still computationally less intensive compared to multiple linear regression with multiple features.
Multiple Linear Regression deals with many variables (X_1, X_2, ..., X_n), fitting a hyperplane in higher dimensions, which is computationally more intensive than polynomial regression that fits a curve in two dimensions.
3. Bias-Variance Trade-off
Bias: The error due to overly simplistic models (e.g., fitting a straight line when the relationship is curved). High bias leads to underfitting.
Variance: The error due to model sensitivity, where the model fits the training data very well but performs poorly on unseen data (overfitting).
Trade-off: Increasing the polynomial degree decreases bias but increases variance; decreasing degree increases bias but reduces variance. The goal is to balance these to minimize total error.
Irreducible Error: Noise in data that cannot be mitigated by any model. Total error = Bias² + Variance + Irreducible error.
4. Hyperparameters in Polynomial Regression
The degree of the polynomial is a hyperparameter, which is not learned during training but manually set or tuned by the modeler.
Changing the degree changes the model’s complexity and fit.
There is no automatic way for the model to pick the perfect degree; it depends on the problem, tolerance for error, and compute resources.
5. Data Splitting: Training, Validation, and Testing Sets
To prevent overfitting and biased model evaluation, data is split into:
Training set: Used for learning model parameters.
Validation set: Used to tune hyperparameters like polynomial degree without touching the test data.
Testing set: Used for final evaluation to check how well the model generalizes to unseen data.
Validation helps decide the best polynomial degree by observing validation error.
6. Overfitting and Underfitting Examples
Overfitting: Polynomial degree too high (e.g., degree 10 with few data points) fits every training point perfectly (zero training error), but performs poorly on new data (high validation/test error).
Underfitting: Polynomial degree too low (e.g., degree 1, simple linear regression) cannot capture data pattern, leading to high bias and poor fit.
Proper model complexity lies at the "sweet spot" of the U-shaped bias-variance curve, minimizing total error.
7. Practical Example Using Python (sklearn)
Introduced PolynomialFeatures to transform input data for polynomial regression.
Demonstrated fitting models with degrees 1 to 5, observing mean square error decline with increasing degree.
Highlighted that while training MSE decreases with complexity, validation MSE starts to increase past a certain degree, indicating overfitting.
Showed predictions from linear vs polynomial models, with polynomial better capturing nonlinear trends.
8. Additional Concepts
Collinearity in Polynomial Features: Powers of the same variable are correlated, causing multicollinearity problems similar to those in multiple regression.
Runge’s Phenomenon: High-degree polynomials oscillate near boundaries, leading to unstable predictions.
Choosing Degree with Limited Data: High-degree polynomials are unsuitable for small datasets—prefer low degrees in such cases.
Importance of Generalization: Aim to minimize error on validation/test data, not just training data, to avoid overfitting.
Summary
Polynomial regression extends linear regression by including powers of a single feature, remaining linear in weights thus computationally manageable.
The degree of polynomial is a critical hyperparameter that controls the trade-off between bias and variance.
Proper model selection requires balancing complexity to avoid underfitting and overfitting, using training and validation splits.
More data points help reduce variance and enable fitting higher-degree polynomials safely.
Validation set helps tune polynomial degree without leaking test information.
Practical implementation using sklearn’s PolynomialFeatures and linear regression is straightforward.
Overfitting and underfitting remain key challenges, best addressed by tuning model complexity informed by validation performance.