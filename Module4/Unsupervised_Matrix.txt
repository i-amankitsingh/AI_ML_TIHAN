Centroid-Based Clustering & Unsupervised Metrics: Lecture Notes
Prerequisites: Comfortable with Python/NumPy, distance metrics, and basic statistics (means, variance, standardization).

What you'll master:

Describe the K-Means objective function, its optimization loop, and why initialization matters.
Implement an end-to-end clustering workflow (scaling → modelling → evaluation → interpretation) in Python.
Compare internal metrics (inertia, silhouette, Calinski–Harabasz) and explain what each reveals about cluster quality.
Communicate clustering outcomes responsibly, including limits and follow-up analyses.
1. Why Centroid-Based Clustering Matters
1.1 Real-World Problem Framing
Imagine you lead analytics for a subscription app. You have usage logs but no labels. Marketing wants audience segments, product wants to group features by adoption patterns, and support wants to flag risky accounts. Centroid-based clustering—most notably K-Means—creates compact groups by representing each cluster with its centroid (mean point). These centroids become “personas” that business teams can act on.

1.2 Analogy: Sorting Marbles by Gravity
Think of dropping marbles on a table with pits. Each marble rolls to the nearest pit; over time, marbles around the same pit form a cluster. The pits are like centroids: positions that attract similar points. However, if the table is tilted (features on different scales) or the pits have odd shapes (non-spherical clusters), marbles collect incorrectly—mirroring K-Means’ sensitivity to scale and shape.

1.3 When to Use (and Not Use) K-Means
Good Fit	Poor Fit
Numerical features scaled similarly	Heavy categorical or binary features without embeddings
Goal: compact, roughly spherical clusters	Highly elongated or manifold-shaped clusters (e.g., concentric circles)
Need for speed on large datasets	Situations where cluster count should emerge automatically
Rule of thumb: Use K-Means when clusters are expected to be “clouds” of points with similar variance in every direction and when speed/interpretability trump flexibility.

2. Core Mechanics of K-Means
2.1 Optimization Objective
K-Means minimizes the within-cluster sum of squares (WCSS), often called inertia:

[ J = \sum_{i=1}^{K} \sum_{x_j \in C_i} \lVert x_j - \mu_i Vert_2^2 ]

(C_i): set of points assigned to cluster (i)
(\mu_i): centroid (mean vector) of cluster (i)
Distance metric is Euclidean by default (L2 norm).
This objective prefers tight, compact clusters. It does not penalize overlapping clusters or reward separation explicitly—that’s why we complement inertia with other metrics.

2.2 Alternating Minimization Loop
Assignment step: Assign each point to the nearest centroid.
Update step: Recalculate each centroid as the mean of its assigned points.
Repeat until assignments no longer change or improvement falls below a tolerance.
This procedure monotonically decreases inertia and converges in a finite number of steps. However, it can land on a local optimum depending on initial centroid placement.

2.3 Initialization Strategies
Random: Fast, but vulnerable to bad starting points.
K-Means++: Probabilistic seeding that spreads centroids apart; reduces variance and usually preferred. Set init="k-means++" (default in scikit-learn).
Domain-informed: Seed centroids using domain knowledge (e.g., known archetypes) when available.
Always try multiple initializations (n_init). In scikit-learn ≥1.4, using n_init="auto" adapts the number of restarts based on data dimensionality.

2.4 Complexity & Scaling
Each iteration costs roughly (O(nkd)) (n samples, k clusters, d features). For large datasets, consider MiniBatchKMeans, which uses small random batches to approximate updates and drastically cuts runtime with modest accuracy trade-offs.

3. From Data to Clusters: Workflow Walkthrough
We’ll follow a repeatable pipeline using scikit-learn utilities.

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.datasets import make_blobs
import pandas as pd
import numpy as np

# 1. Generate toy data (replace with real dataset)
X, _ = make_blobs(n_samples=600, centers=4, cluster_std=[1.0, 2.0, 1.5, 0.8], random_state=42)

# 2. Build pipeline: scaling + clustering
model = Pipeline([
    ("scaler", StandardScaler()),
    ("kmeans", KMeans(n_clusters=4, init="k-means++", n_init="auto", random_state=42))
])

model.fit(X)
labels = model["kmeans"].labels_
centroids = model["kmeans"].cluster_centers_
inertia = model["kmeans"].inertia_
silhouette = silhouette_score(model["scaler"].transform(X), labels)
ch_index = calinski_harabasz_score(model["scaler"].transform(X), labels)

summary = pd.DataFrame({
    "cluster": range(model["kmeans"].n_clusters),
    "count": np.bincount(labels)
})
Interpretation Pointers
Inspect centroids: centroids are in scaled space. Use model.named_steps['scaler'].inverse_transform(centroids) to map them back to original units before presenting to stakeholders.
Check inertia: Lower values indicate tighter clusters. Compare across different K values rather than absolute numbers.
Evaluate silhouette/Calinski–Harabasz: Silhouette (−1 to 1) balances cohesion/separation. Calinski–Harabasz increases with better-defined clusters.
Profile clusters: Join labels back to original DataFrame; compute per-cluster averages, medians, counts, and key business KPIs.
Visual Diagnostics
Elbow plot: Plot inertia vs K to spot diminishing returns.
Silhouette plot: For the selected K, draw bar plots of silhouette values per cluster to detect poorly assigned points.
2D projections: Use PCA/UMAP to visualize clusters and centroids; annotate for narratives.
4. Internal Validation Metrics Deep Dive
Metric	Measures	Range	When to use	Gotchas
Inertia (WCSS)	Compactness within clusters	([0, \infty))	Quick heuristic, elbow method	Always decreases with higher K; scale-dependent
Silhouette Score	Cohesion vs separation	([-1, 1])	Comparing cluster counts, diagnosing overlap	Requires distance matrix; expensive for very large n
Calinski–Harabasz	Variance ratio (between vs within)	((0, \infty))	Fast alternative to silhouette	Inflated by higher-dimensional data
Davies–Bouldin	Average similarity between clusters	([0, \infty))	Another quick internal metric	Lower is better; sensitive to noisy clusters
Tip: Combine multiple metrics and favor consistency over single metric maxima. If silhouette and Calinski–Harabasz disagree, inspect cluster shapes directly.

Worked Example: Manual Silhouette for One Point
Given point (x) assigned to cluster (A):

Compute (a): average distance from (x) to all other points in cluster (A).
Compute (b): smallest average distance from (x) to points in any other cluster.
Silhouette (s = (b - a)/\max(a,b)).
High (s) (≈0.7–1) → well-placed. Near 0 → on boundary. Negative → probably misclassified.

Stability Checks
Run clustering multiple times with different seeds; compute Adjusted Rand Index (ARI) between label sets. Instability suggests either K is too high, or true structure is weak.
For time-series or recurring reports, compare clusters across different time slices to ensure narratives persist.
5. Interpreting and Presenting Clusters
5.1 Translate Numbers into Personas
Create a cluster summary table:

Cluster	Size	Key Features (mean ± std)	Interpretation	Action Idea
0	245	Spend: $120 ± 15, Visits: 3/week	“Budget Loyalists”	Offer bundle discount
1	180	Spend: $310 ± 40, Visits: 5/week	“High-Value Regulars”	Launch premium loyalty plan
5.2 Stress-Testing Interpretations
Overlaps: Examine pairwise feature plots; if clusters overlap heavily, question usefulness.
Outliers: Identify clusters with tiny membership—are they outliers worth separate handling or noise that should be removed? (Maybe re-run with K-1 or apply robust methods).
Business validation: Share preliminary clusters with domain experts; sanity-check whether behaviors align with expectations.
5.3 Communicating Uncertainty
Always accompany personas with caveats:

Clusters are algorithmic suggestions; they may shift when new data arrives.
Metrics do not guarantee business value—pilot experiments needed.
Document preprocessing steps (scaling, feature selection) so findings are reproducible.
6. Common Pitfalls & How to Avoid Them
Mixing scales: Forgetting to scale features leads to centroids dominated by large-magnitude units. Fix: Standardize or normalize before clustering.
Too many clusters: Chasing metric improvements yields fragmented clusters with little meaning. Fix: Balance metrics with interpretability; merge clusters that lack distinct behaviors.
Ignoring feature correlations: Highly correlated features can skew distance calculations. Fix: Apply PCA or decorrelate features; inspect correlation matrix.
Blindly trusting inertia: Low inertia ≠ useful clusters. Fix: Pair inertia with silhouette, CH, and manual inspection.
Static segmentation: Assuming cluster labels are timeless. Fix: Refit periodically; monitor drift and retrain when cluster assignments change significantly.
Treating K-Means as one-size-fits-all: Using it on categorical or non-convex clusters (e.g., concentric circles). Fix: Consider K-Medoids, DBSCAN, or Gaussian Mixtures when assumptions break.
7. Practice Lab (for Workshop/Assignment)
Use these prompts to reinforce concepts before tackling the graded assignment:

Elbow Exploration: Run K-Means for K=2…10 on a chosen dataset. Record inertia and silhouette; write a short justification for your chosen K.
Silhouette Dissection: Plot silhouette diagrams for the candidate Ks. Identify clusters with many negative silhouettes and explain potential causes.
Centroid Personas: Inverse-transform centroid coordinates; create short persona statements (“Cluster A: infrequent visitors, low spend, high churn risk”).
Stability Check: Repeat clustering with five different seeds; compute ARI. If results vary drastically, decide on mitigation (more data? different K? alternate algorithm?).
Alternative Algorithm: Compare K-Means with MiniBatchKMeans or GaussianMixture. Document differences in metrics, runtime, and interpretability.
Encourage learners to capture lessons in a short reflective note: “What surprised you about the metrics? When did clusters stop making sense?”

8. Suggested References & Further Reading
scikit-learn User Guide: sklearn.cluster.KMeans, MiniBatchKMeans, and metrics modules.
“The Elements of Statistical Learning” (Hastie et al.) – Chapter on clustering for theoretical foundations.
StatQuest (YouTube): Explainers on K-Means, silhouette score, and elbow method.
Practical blog posts:
“How to evaluate clustering algorithms” (Towards Data Science)
“Interpreting K-Means cluster centroids in business analytics” (O’Reilly Radar)
Advanced topics to explore next: soft clustering with Gaussian Mixture Models, density-based clustering (DBSCAN), automatic selection of K (gap statistic, Bayesian non-parametrics).
Document experiments, keep notebooks version-controlled, and tie every cluster back to a concrete decision to ensure these techniques drive value beyond the classroom.