End-to-End House Price Prediction: Regression Case Study
Prerequisites: Basic Python programming, understanding of pandas and numpy, familiarity with supervised learning concepts

Time to complete: 60-90 minutes

What you'll learn:

Complete workflow from raw data to deployed predictions
Data preprocessing and feature engineering techniques
Model training, evaluation, and comparison strategies
Extracting business insights from model results
1. Introduction: The Business Problem
The Challenge
A real estate company wants to automate property valuation to help agents price homes accurately and quickly. Manual valuations are time-consuming, inconsistent across agents, and sometimes lead to overpriced listings that don't sell or underpriced properties that lose money.

Our Goal
Build a machine learning system that predicts house prices based on property characteristics. The system should:

Predict prices within 10% accuracy for 80% of properties
Identify which features most influence price
Provide interpretable results that agents can explain to clients
Why This Matters
Accurate automated valuations enable:

Faster pricing decisions (minutes vs. days)
Consistent valuations across all agents
Data-driven negotiations with buyers/sellers
Better inventory management and marketing strategies
2. Data: Understanding What We Have
Dataset Overview
We're working with historical home sales data containing:

5,000 properties sold in the last 2 years
15 features describing each property
Target variable: Sale price (in dollars)
Feature Descriptions
Feature	Type	Description	Example Values
square_feet	Numeric	Total living area	1,200 - 4,500
bedrooms	Numeric	Number of bedrooms	1 - 6
bathrooms	Numeric	Number of bathrooms	1.0 - 4.5
lot_size	Numeric	Land area in sq ft	2,000 - 20,000
year_built	Numeric	Construction year	1950 - 2023
garage_spaces	Numeric	Parking capacity	0 - 3
neighborhood	Categorical	Area name	Downtown, Suburb, Rural
property_type	Categorical	House style	Single-family, Condo, Townhouse
condition	Categorical	Overall state	Excellent, Good, Fair, Poor
has_pool	Binary	Pool present	0 or 1
has_ac	Binary	Air conditioning	0 or 1
basement_sqft	Numeric	Finished basement area	0 - 1,500
stories	Numeric	Number of floors	1 - 3
distance_downtown	Numeric	Miles from center	0.5 - 25
school_rating	Numeric	Nearby school score	1 - 10
Initial Data Exploration
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
df = pd.read_csv('house_prices.csv')

# Basic info
print(f"Dataset shape: {df.shape}")
print(f"\nFirst few rows:")
print(df.head())

# Check for missing values
print(f"\nMissing values:")
print(df.isnull().sum())

# Summary statistics
print(f"\nPrice statistics:")
print(df['price'].describe())

# Visualize price distribution
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.hist(df['price'], bins=50, edgecolor='black')
plt.xlabel('Price ($)')
plt.ylabel('Frequency')
plt.title('Price Distribution')

plt.subplot(1, 2, 2)
plt.hist(np.log(df['price']), bins=50, edgecolor='black')
plt.xlabel('Log(Price)')
plt.ylabel('Frequency')
plt.title('Log-Transformed Price Distribution')
plt.tight_layout()
plt.show()

Key Observations:

Price ranges from $150,000 to $1,200,000 (right-skewed distribution)
Missing values inbasement_sqft(20% of records) andgarage_spaces(5%)
Strong correlation betweensquare_feetand price (r = 0.78)
Categorical features need encoding
Log transformation makes price distribution more normal
3. Data Preprocessing: Preparing for Modeling
Step 1: Handle Missing Values
# Strategy for missing values
# basement_sqft: Missing likely means no basement → fill with 0
df['basement_sqft'] = df['basement_sqft'].fillna(0)

# garage_spaces: Missing is truly unknown → fill with median
df['garage_spaces'] = df['garage_spaces'].fillna(df['garage_spaces'].median())

# Verify no missing values remain
print(df.isnull().sum().sum())  # Should be 0

Why this approach:

Basement: Absence of data indicates absence of feature (structural zeros)
Garage: Missing at random, median preserves distribution without introducing bias
Step 2: Feature Engineering
# Create new features that might improve predictions

# 1. Property age (more intuitive than year_built)
current_year = 2024
df['age'] = current_year - df['year_built']

# 2. Total bathrooms (combining full and half baths)
# Already in dataset as decimal (2.5 = 2 full + 1 half)

# 3. Price per square foot (target-derived, for analysis only)
df['price_per_sqft'] = df['price'] / df['square_feet']

# 4. Total interior space
df['total_interior_sqft'] = df['square_feet'] + df['basement_sqft']

# 5. Bathroom-to-bedroom ratio
df['bath_bed_ratio'] = df['bathrooms'] / (df['bedrooms'] + 1)  # +1 to avoid division by zero

# 6. Is the property new? (built in last 5 years)
df['is_new'] = (df['age'] <= 5).astype(int)

# 7. Has basement indicator
df['has_basement'] = (df['basement_sqft'] > 0).astype(int)

print("New features created:")
print(df[['age', 'total_interior_sqft', 'bath_bed_ratio', 'is_new', 'has_basement']].head())

Feature engineering rationale:

Age: Models learn patterns better with "years old" than "year built"
Total interior: Combined living space might predict better than separate areas
Ratios: Relative proportions often matter more than absolute counts
Binary flags: Capture threshold effects (new construction premium, basement value)
Step 3: Encode Categorical Variables
from sklearn.preprocessing import LabelEncoder

# Method 1: One-Hot Encoding for nominal categories (no order)
df_encoded = pd.get_dummies(df, columns=['neighborhood', 'property_type'],
                            drop_first=True)

# Method 2: Ordinal Encoding for ordered categories
condition_mapping = {'Poor': 1, 'Fair': 2, 'Good': 3, 'Excellent': 4}
df_encoded['condition_encoded'] = df['condition'].map(condition_mapping)

print(f"Shape after encoding: {df_encoded.shape}")
print(f"New columns: {[col for col in df_encoded.columns if col not in df.columns]}")

Encoding decisions:

One-hot encoding: For neighborhood and property_type (no inherent order)
Ordinal encoding: For condition (clear ordering from poor to excellent)
drop_first=True: Prevents multicollinearity (if not Downtown or Suburb, must be Rural)
Step 4: Feature Scaling
from sklearn.preprocessing import StandardScaler

# Separate features that need scaling
numeric_features = ['square_feet', 'lot_size', 'age', 'distance_downtown',
                   'bathroom', 'bedrooms', 'garage_spaces', 'basement_sqft',
                   'total_interior_sqft', 'school_rating']

# Note: We'll scale AFTER train-test split to prevent data leakage
# This is just identifying which features need scaling

Why scale:

Distance-based models (KNN) and regularized models (Ridge, Lasso) are sensitive to feature magnitudes
Tree-based models (Random Forest) don't need scaling but it doesn't hurt
Neural networks require scaling for stable training
4. Model Building: Training and Comparison
Step 1: Prepare Train-Test Split
from sklearn.model_selection import train_test_split

# Separate features from target
X = df_encoded.drop(['price', 'price_per_sqft'], axis=1)  # Remove target and derived feature
y = df_encoded['price']

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# Now scale features (fit on train only!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[numeric_features])
X_test_scaled = scaler.transform(X_test[numeric_features])

# Replace scaled values back into dataframes
X_train_final = X_train.copy()
X_test_final = X_test.copy()
X_train_final[numeric_features] = X_train_scaled
X_test_final[numeric_features] = X_test_scaled

Critical point: Scaling is fit only on training data, then applied to test data using the same transformation. This simulates real-world deployment where future data uses the same scaling parameters.

Step 2: Train Multiple Models
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import time

# Dictionary to store models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=10),
    'Lasso Regression': Lasso(alpha=10),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
}

# Train and evaluate each model
results = []

for name, model in models.items():
    # Train
    start_time = time.time()
    model.fit(X_train_final, y_train)
    train_time = time.time() - start_time

    # Predict
    y_pred = model.predict(X_test_final)

    # Evaluate
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

    results.append({
        'Model': name,
        'MAE': mae,
        'RMSE': rmse,
        'R²': r2,
        'MAPE (%)': mape,
        'Train Time (s)': train_time
    })

# Display results
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('MAE')
print(results_df.to_string(index=False))

Expected output:

              Model       MAE      RMSE    R²  MAPE (%)  Train Time (s)
  Gradient Boosting  18542.31  28341.22  0.91      4.23           2.14
      Random Forest  19823.45  29876.54  0.90      4.51           1.87
  Ridge Regression  24567.89  35432.11  0.86      5.67           0.03
 Linear Regression  24891.23  35789.45  0.85      5.78           0.02
  Lasso Regression  26234.56  37123.89  0.84      6.12           0.04

Step 3: Cross-Validation for Robustness
from sklearn.model_selection import cross_val_score

# Use cross-validation on the best model (Gradient Boosting)
best_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)

# 5-fold cross-validation
cv_scores = cross_val_score(best_model, X_train_final, y_train,
                           cv=5, scoring='neg_mean_absolute_error')

cv_mae = -cv_scores.mean()
cv_std = cv_scores.std()

print(f"Cross-Validation MAE: ${cv_mae:,.2f} (+/- ${cv_std:,.2f})")

Why cross-validation:

Ensures model performance is consistent across different data subsets
Reduces risk of lucky/unlucky train-test split
Provides confidence intervals for performance estimates
5. Evaluation: Understanding Model Performance
Metric Interpretation
# Detailed evaluation of best model
best_model.fit(X_train_final, y_train)
y_pred = best_model.predict(X_test_final)

# Calculate all metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

print("=== MODEL PERFORMANCE ===")
print(f"MAE: ${mae:,.2f}")
print(f"  → Typical prediction is off by ${mae:,.0f}")
print(f"\nRMSE: ${rmse:,.2f}")
print(f"  → Penalizes large errors more than MAE")
print(f"\nR²: {r2:.4f}")
print(f"  → Model explains {r2*100:.2f}% of price variance")
print(f"\nMAPE: {mape:.2f}%")
print(f"  → Average error is {mape:.1f}% of actual price")

# Check if we met the business goal (10% accuracy for 80% of properties)
percentage_errors = np.abs((y_test - y_pred) / y_test) * 100
within_10pct = (percentage_errors <= 10).sum() / len(y_test) * 100
print(f"\n✓ {within_10pct:.1f}% of predictions within 10% of actual price")
print(f"  Goal: 80% → {'ACHIEVED' if within_10pct >= 80 else 'NOT MET'}")

Visualizing Predictions
# Plot predicted vs actual prices
plt.figure(figsize=(12, 5))

# Subplot 1: Scatter plot
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
         'r--', lw=2, label='Perfect Prediction')
plt.xlabel('Actual Price ($)')
plt.ylabel('Predicted Price ($)')
plt.title('Predicted vs Actual Prices')
plt.legend()

# Subplot 2: Residual plot
plt.subplot(1, 2, 2)
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--', lw=2)
plt.xlabel('Predicted Price ($)')
plt.ylabel('Residual (Actual - Predicted)')
plt.title('Residual Plot')

plt.tight_layout()
plt.show()

# Analyze residuals
print("\n=== RESIDUAL ANALYSIS ===")
print(f"Mean residual: ${residuals.mean():,.2f}")
print(f"Std of residuals: ${residuals.std():,.2f}")

# Check for patterns in errors
over_predictions = (residuals < -50000).sum()
under_predictions = (residuals > 50000).sum()
print(f"\nLarge errors (>$50k):")
print(f"  Over-predictions: {over_predictions}")
print(f"  Under-predictions: {under_predictions}")

Error Analysis by Segments
# Analyze performance by price range
price_bins = pd.qcut(y_test, q=4, labels=['Low', 'Medium', 'High', 'Very High'])
error_by_price = pd.DataFrame({
    'Price_Range': price_bins,
    'Actual': y_test,
    'Predicted': y_pred,
    'Error': np.abs(y_test - y_pred),
    'Pct_Error': np.abs((y_test - y_pred) / y_test) * 100
})

print("\n=== PERFORMANCE BY PRICE RANGE ===")
print(error_by_price.groupby('Price_Range').agg({
    'Error': 'mean',
    'Pct_Error': 'mean'
}).round(2))

# Analyze by neighborhood
neighborhood_cols = [col for col in X_test_final.columns if 'neighborhood_' in col]
if neighborhood_cols:
    for col in neighborhood_cols:
        neighborhood_mask = X_test_final[col] == 1
        if neighborhood_mask.sum() > 0:
            neighborhood_mae = mean_absolute_error(
                y_test[neighborhood_mask],
                y_pred[neighborhood_mask]
            )
            print(f"{col}: MAE = ${neighborhood_mae:,.2f}")

6. Insights: Feature Importance and Business Implications
Feature Importance Analysis
# Get feature importances from Gradient Boosting
feature_importance = pd.DataFrame({
    'Feature': X_train_final.columns,
    'Importance': best_model.feature_importances_
}).sort_values('Importance', ascending=False)

# Plot top 15 features
plt.figure(figsize=(10, 8))
top_features = feature_importance.head(15)
plt.barh(range(len(top_features)), top_features['Importance'])
plt.yticks(range(len(top_features)), top_features['Feature'])
plt.xlabel('Importance Score')
plt.title('Top 15 Most Important Features')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("\n=== TOP 10 MOST IMPORTANT FEATURES ===")
print(feature_importance.head(10).to_string(index=False))

Expected insights:

         Feature  Importance
   square_feet      0.3245
   distance_downtown 0.1823
   age              0.1156
   school_rating    0.0987
   bathrooms        0.0745
   neighborhood_Downtown 0.0623
   total_interior_sqft   0.0512
   lot_size         0.0445
   bedrooms         0.0398
   has_pool         0.0321

Business Insights
# Insight 1: Price sensitivity to key features
print("\n=== PRICE SENSITIVITY ANALYSIS ===")

# Calculate average price change per feature unit
for feature in ['square_feet', 'age', 'bathrooms', 'school_rating']:
    # Simple linear approximation
    feature_values = X_train_final[feature]
    correlation = np.corrcoef(feature_values, y_train)[0, 1]

    # Estimate dollar impact (rough approximation)
    price_std = y_train.std()
    feature_std = feature_values.std()
    dollar_impact = correlation * (price_std / feature_std)

    print(f"\n{feature}:")
    print(f"  Correlation with price: {correlation:.3f}")
    print(f"  Estimated impact: ${dollar_impact:,.2f} per unit")

# Insight 2: Neighborhood premiums
print("\n=== NEIGHBORHOOD PRICE PREMIUMS ===")
neighborhood_cols = [col for col in X_train_final.columns if 'neighborhood_' in col]
for col in neighborhood_cols:
    mask = X_train_final[col] == 1
    avg_price = y_train[mask].mean()
    overall_avg = y_train.mean()
    premium = ((avg_price - overall_avg) / overall_avg) * 100
    print(f"{col.replace('neighborhood_', '')}: {premium:+.1f}%")

# Insight 3: Optimal property profiles
print("\n=== HIGH-VALUE PROPERTY CHARACTERISTICS ===")
top_20pct_mask = y_test >= y_test.quantile(0.8)
top_properties = X_test_final[top_20pct_mask]

print("Average characteristics of top 20% priced homes:")
for feature in ['square_feet', 'age', 'bathrooms', 'bedrooms', 'school_rating']:
    avg_top = top_properties[feature].mean()
    avg_all = X_test_final[feature].mean()
    print(f"  {feature}: {avg_top:.1f} (vs {avg_all:.1f} overall)")

Actionable Recommendations
print("\n" + "="*60)
print("ACTIONABLE BUSINESS RECOMMENDATIONS")
print("="*60)

recommendations = """
1. PRICING STRATEGY:
   • Square footage is the #1 price driver (32% importance)
   • Every 100 sq ft adds approximately $15,000 in value
   • Focus marketing on space when pricing premium properties

2. INVESTMENT PRIORITIES:
   • School rating has 10% importance in model
   • Properties near top-rated schools (8-10 rating) command
     15-20% premium
   • Target acquisitions in high-school-rating areas

3. RENOVATION ROI:
   • Adding bathrooms: High impact (7.5% importance)
   • Basement finishing: Moderate impact (via total_interior_sqft)
   • Pool installation: Lower impact (3.2% importance)
   • Recommendation: Prioritize bathroom additions over pools

4. NEIGHBORHOOD DYNAMICS:
   • Downtown properties: +25% premium
   • Suburban properties: baseline
   • Rural properties: -15% discount
   • Adjust marketing spend accordingly

5. MODEL DEPLOYMENT:
   • Achieved 85% of predictions within 10% accuracy
   • Exceeds 80% target → Ready for production
   • Recommend monthly retraining as new sales data arrives
   • Flag predictions with >15% uncertainty for manual review

6. COMPETITIVE ADVANTAGE:
   • Automate initial valuations (saves 2-3 hours per property)
   • Consistent pricing across all agents
   • Data-driven negotiation support
   • Estimated time savings: $50,000/year in agent hours
"""

print(recommendations)

7. Model Deployment: Productionizing the Solution
Save the Model
import joblib

# Save the trained model
joblib.dump(best_model, 'house_price_model.pkl')

# Save the scaler
joblib.dump(scaler, 'feature_scaler.pkl')

# Save feature names for reference
feature_names = X_train_final.columns.tolist()
joblib.dump(feature_names, 'feature_names.pkl')

print("Model saved successfully!")

Create Prediction Function
def predict_house_price(property_features):
    """
    Predict house price given property characteristics.

    Args:
        property_features (dict): Dictionary containing property features

    Returns:
        float: Predicted price in dollars
    """
    # Load saved artifacts
    model = joblib.load('house_price_model.pkl')
    scaler = joblib.load('feature_scaler.pkl')
    feature_names = joblib.load('feature_names.pkl')

    # Convert input to DataFrame
    input_df = pd.DataFrame([property_features])

    # Apply same preprocessing as training
    # (handle missing values, encoding, feature engineering, scaling)

    # Make prediction
    predicted_price = model.predict(input_df)[0]

    return predicted_price

# Example usage
new_property = {
    'square_feet': 2500,
    'bedrooms': 4,
    'bathrooms': 2.5,
    'age': 10,
    'neighborhood': 'Suburb',
    'has_pool': 1,
    'school_rating': 8,
    # ... other features
}

estimated_price = predict_house_price(new_property)
print(f"Estimated price: ${estimated_price:,.2f}")

Monitoring Plan
monitoring_plan = """
POST-DEPLOYMENT MONITORING PLAN:

1. Performance Tracking:
   - Log all predictions and actual sale prices
   - Calculate rolling 30-day MAE, MAPE
   - Alert if MAPE exceeds 7% threshold

2. Data Drift Detection:
   - Monitor feature distributions monthly
   - Compare incoming data to training distribution
   - Flag significant shifts (>2 std deviations)

3. Retraining Schedule:
   - Automatic retraining: Monthly
   - Emergency retraining: If performance degrades >10%
   - Minimum new data: 200 sales before retraining

4. Edge Case Handling:
   - Flag luxury properties (>$1M) for manual review
   - Flag rural properties (>20 miles from downtown)
   - Flag unusual features (>6 bedrooms, built before 1900)

5. User Feedback Loop:
   - Collect agent feedback on predictions
   - Track which predictions agents override
   - Use feedback to improve model in next iteration
"""

print(monitoring_plan)

8. Summary: Key Takeaways
What We Accomplished
✅ Data Processing:

Handled missing values strategically (structural zeros vs. random missing)
Engineered 6 new features to capture domain knowledge
Encoded categorical variables appropriately
Scaled features to prevent magnitude bias
✅ Modeling:

Trained and compared 5 different algorithms
Selected Gradient Boosting as best performer (MAE = $18,542, R² = 0.91)
Validated robustness with cross-validation
Achieved business goal: 85% of predictions within 10% accuracy
✅ Insights:

Identified square footage as #1 price driver (32% importance)
Quantified neighborhood premiums (Downtown: +25%)
Provided ROI guidance for renovations (bathrooms > pools)
Demonstrated $50K/year cost savings opportunity
✅ Deployment:

Saved production-ready model and preprocessing pipeline
Created prediction function for integration
Established monitoring plan for ongoing performance
Lessons Learned
Technical:

Feature engineering matters more than algorithm choice
Proper train-test split prevents overfitting
Multiple metrics provide complete performance picture
Domain knowledge improves model interpretability
Business:

Explainability builds trust with end users
Segment-specific analysis reveals actionable insights
Clear success metrics align ML with business goals
Monitoring plan is essential for production ML
Next Steps
Immediate (Week 1):

Deploy model to staging environment
Train agents on using predictions
Set up monitoring dashboard
Short-term (Month 1):

Collect feedback from initial usage
Analyze prediction overrides
Refine edge case handling
Long-term (Quarter 1):

Expand to additional markets
Add confidence intervals to predictions
Integrate with CRM system
Build automated reporting
Appendix: Complete Code Walkthrough
Full Pipeline in One Script
# house_price_pipeline.py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import joblib

def run_complete_pipeline(data_path):
    """Execute end-to-end house price prediction pipeline."""

    # 1. LOAD DATA
    print("Step 1: Loading data...")
    df = pd.read_csv(data_path)
    print(f"Loaded {len(df)} properties")

    # 2. DATA PREPROCESSING
    print("\nStep 2: Preprocessing...")

    # Handle missing values
    df['basement_sqft'] = df['basement_sqft'].fillna(0)
    df['garage_spaces'] = df['garage_spaces'].fillna(df['garage_spaces'].median())

    # Feature engineering
    current_year = 2024
    df['age'] = current_year - df['year_built']
    df['total_interior_sqft'] = df['square_feet'] + df['basement_sqft']
    df['bath_bed_ratio'] = df['bathrooms'] / (df['bedrooms'] + 1)
    df['is_new'] = (df['age'] <= 5).astype(int)
    df['has_basement'] = (df['basement_sqft'] > 0).astype(int)

    # Encode categorical variables
    condition_map = {'Poor': 1, 'Fair': 2, 'Good': 3, 'Excellent': 4}
    df['condition_encoded'] = df['condition'].map(condition_map)
    df = pd.get_dummies(df, columns=['neighborhood', 'property_type'], drop_first=True)

    # 3. PREPARE TRAIN-TEST SPLIT
    print("\nStep 3: Splitting data...")
    X = df.drop(['price'], axis=1)
    y = df['price']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Scale numeric features
    numeric_features = ['square_feet', 'lot_size', 'age', 'distance_downtown',
                       'bathrooms', 'bedrooms', 'garage_spaces', 'basement_sqft',
                       'total_interior_sqft', 'school_rating']

    scaler = StandardScaler()
    X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])
    X_test[numeric_features] = scaler.transform(X_test[numeric_features])

    # 4. TRAIN MODEL
    print("\nStep 4: Training model...")
    model = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
    model.fit(X_train, y_train)

    # 5. EVALUATE
    print("\nStep 5: Evaluating...")
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

    print(f"MAE: ${mae:,.2f}")
    print(f"R²: {r2:.4f}")
    print(f"MAPE: {mape:.2f}%")

    # 6. SAVE MODEL
    print("\nStep 6: Saving model...")
    joblib.dump(model, 'house_price_model.pkl')
    joblib.dump(scaler, 'feature_scaler.pkl')
    joblib.dump(X.columns.tolist(), 'feature_names.pkl')

    print("\n✓ Pipeline complete!")

    return model, scaler, mae, r2

# Run the pipeline
if __name__ == "__main__":
    model, scaler, mae, r2 = run_complete_pipeline('house_prices.csv')

Practice Exercise
Challenge: Apply this pipeline to a new dataset with apartment rentals.

Dataset: Monthly rent prices with features:

square_meters: 30-150 m²
bedrooms: 1-4
floor: 1-20
has_elevator: 0 or 1
distance_center: 0-15 km
year_built: 1960-2023
has_parking: 0 or 1
neighborhood: Categorical
rent: Target variable (monthly rent)
Tasks:

Load and explore the data
Create at least 3 derived features
Handle any missing values appropriately
Train Linear Regression, Ridge, and Random Forest models
Compare performance using MAE and R²
Identify top 5 most important features
Provide 3 business recommendations based on findings
Starter code:

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Your code here
df = pd.read_csv('apartment_rentals.csv')

# TODO: Complete the analysis following the house price example

Additional Resources
Further Reading
Scikit-learn Documentation: Regression Metrics
Feature Engineering Guide: Kaggle Learn - Feature Engineering course
Model Interpretability: SHAP (SHapley Additive exPlanations) library for advanced feature importance
Common Questions
Q: Why use MAPE instead of just MAE? A: MAPE (Mean Absolute Percentage Error) makes errors comparable across different price ranges. A $20K error matters more for a $200K house than a $1M house.

Q: When should I use Ridge vs Lasso regression? A: Ridge when you want to keep all features but penalize large coefficients. Lasso when you want automatic feature selection (it can zero out unimportant features).

Q: How often should I retrain the model? A: Depends on how quickly your market changes. For real estate: monthly is typical. Monitor performance metrics—if they degrade significantly, retrain sooner.

Q: What if my predictions are consistently too high or too low? A: This indicates bias. Check if your training data represents the full range of properties. You might need to collect more diverse examples or adjust your features.

Q: How do I handle outliers? A: Options: (1) Remove extreme outliers (>3 std deviations), (2) Log-transform the target variable, (3) Use robust models like Random Forest, or (4) Train separate models for different price tiers.

End of Case Study

Classroom notes shared by the Professor: Kindly find the lecture PPT and the .ipynb file in the attached zip file -
Regression_Diamond_use_case